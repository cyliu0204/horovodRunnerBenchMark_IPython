{"cells":[{"cell_type":"code","source":["dbutils.fs.put(\"tmp/tf_horovod/tf_dbfs_timeout_fix.sh\",\"\"\"\n#!/bin/bash\n\nfusermount -u /dbfs\nnohup /databricks/spark/scripts/fuse/goofys-dbr -f -o allow_other --file-mode=0777 --dir-mode=0777 --type-cache-ttl 0 --stat-cache-ttl 1s --http-timeout 5m /: /dbfs >& /databricks/data/logs/dbfs_fuse_stderr &\"\"\", True)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Wrote 242 bytes.\nOut[2]: True</div>"]}}],"execution_count":1},{"cell_type":"code","source":["%sh\n\nfusermount -u /dbfs\nnohup /databricks/spark/scripts/fuse/goofys-dbr -f -o allow_other --file-mode=0777 --dir-mode=0777 --type-cache-ttl 0 --stat-cache-ttl 1s --http-timeout 5m /: /dbfs >& /databricks/data/logs/dbfs_fuse_stderr &"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":2},{"cell_type":"code","source":["dbutils.fs.head(\"dbfs:/tmp/tf_horovod/tf_dbfs_timeout_fix.sh\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[2]: &#39;\\n#!/bin/bash\\n\\nfusermount -u /dbfs\\nnohup /databricks/spark/scripts/fuse/goofys-dbr -f -o allow_other --file-mode=0777 --dir-mode=0777 --type-cache-ttl 0 --stat-cache-ttl 1s --http-timeout 5m /: /dbfs &gt;&amp; /databricks/data/logs/dbfs_fuse_stderr &amp;&#39;</div>"]}}],"execution_count":3},{"cell_type":"code","source":["### get current timestamp\nimport datetime\nfrom pytz import timezone\n\ncurent_timestamp = datetime.datetime.now().astimezone(timezone('US/Pacific')).strftime(\"%Y_%m_%d_%H_%M_%S\")\n\n### Create horovod_benchmark folder under mounted s3 bucket\nimport os\nfinal_output_dir = \"/dbfs/jingp/horovod_benchmark/logs/\"\noutput_dir = 'logs/logs/horovod_logs/'\n\noutput_dir = output_dir + f\"{curent_timestamp}/\"\n\nfor _path in [output_dir, final_output_dir]:\n  try:\n    os.makedirs(_path)\n    print(f\"{_path} - file created successfully\")\n  except Exception as ex:\n    if FileExistsError:\n      print(f\"{_path} - file already exist\")\n    else:\n      raise ex"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">logs/logs/horovod_logs/2019_12_11_16_00_36/ - file created successfully\n/dbfs/jingp/horovod_benchmark/logs/ - file already exist\n</div>"]}}],"execution_count":4},{"cell_type":"code","source":["import logging\nimport sys\nimport datetime\n\n\n### TO DO \n### ADDING SINGLE_INSTANCE, REPEAT INTO LOGGER STRING FORMAT\noriginal_stdout = sys.stdout\noriginal_stderr = sys.stderr\n\ndef redirect_stdout(log_filename):\n  ### Databricks has weird behavior that need to define redirect stdout in every cell. -- Need follow up with databricks\n  ### https://stackoverflow.com/questions/34248908/how-to-prevent-logging-of-pyspark-answer-received-and-command-to-send-messag\n  ### logging flushed with py4j error messages so to set logging level to ERROR to silence error\n  ### Also leave pyspark/matplotlib silence code just in case\n  logging.getLogger(\"py4j\").setLevel(logging.ERROR)\n  ##logging.getLogger('pyspark').setLevel(logging.ERROR)\n  ##matplotlib_logger = logging.getLogger(\"matplotlib\").setLevel(logging.ERROR)\n  \n  class StreamToLogger:\n    def __init__(self, logger, level):\n        # self.level is really like using log.debug(message)\n        # at least in my case\n        self.level = level\n        self.logger = logger\n\n    def write(self, message):\n        # if statement reduces the amount of newlines that are\n        # printed to the logger\n        if (message != '\\n') & (message != ''):\n          for mess in message.splitlines():\n            self.logger.log(self.level, mess.rstrip())\n            \n    def flush(self):\n      pass\n#     def flush(self):\n#         # create a flush method so things can be flushed when\n#         # the system wants to. Not sure if simply 'printing'\n#         # sys.stderr is the correct way to do it, but it seemed\n#         # to work properly for me.\n#         self.level(sys.stderr)\n        \n  ## Change to pdt timezone (Simply implementation won't consider daylight savings)\n  def pdt_timezone(sec, what):\n    pdt_time = datetime.datetime.now() - datetime.timedelta(hours=8)\n    return pdt_time.timetuple()\n  logging.Formatter.converter = pdt_timezone\n  \n    \n  logging.basicConfig(\n     level=logging.DEBUG,\n     ## format='%(asctime)-%(levelname)s-%(name)s-%(message)s',\n     format=('%(asctime)s - %(name)s - %(levelname)s - %(message)s'),\n     datefmt=\"%Y-%m-%d %H:%M:%S\",\n     filename = log_filename,\n     filemode = 'w'\n  )\n         \n  stdout_logger = logging.getLogger('STDOUT')\n  sl = StreamToLogger(stdout_logger, logging.INFO)\n  sys.stdout = sl\n  \n  stderr_logger = logging.getLogger('STDERR')\n  sle = StreamToLogger(stderr_logger, logging.ERROR)\n  sys.stderr = sle\n  \ndef reset_stdout():\n  sys.stdout = original_stdout\n  sys.stderr = original_stderr\n  temp_logger = logging.getLogger()\n  temp_logger.handlers = []\n  \n  \ndef move_log_to_s3():\n  ### Moving all the driver logs to mounted s3 bucket. \n  import glob\n  from distutils import dir_util \n  ### Shutile will not overwrit the file so use distutils instead\n\n  list_dir = glob.glob(output_dir)\n\n  for path in list_dir:\n    dir_name = os.path.basename(os.path.dirname(path))\n    dest = final_output_dir + dir_name\n    destination = dir_util.copy_tree(path, dest) "],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":5},{"cell_type":"code","source":["dbutils.widgets.removeAll()\ndbutils.widgets.text(name = \"batch_size\", defaultValue = \"128\")\ndbutils.widgets.text(name = \"learning_rate\", defaultValue = \"0.1\")\ndbutils.widgets.text(name = \"epochs\", defaultValue = \"5\")\ndbutils.widgets.text(name = \"repeat\", defaultValue = \"3\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":6},{"cell_type":"code","source":["batch_size = int(dbutils.widgets.get(\"batch_size\"))\nepochs = int(dbutils.widgets.get(\"epochs\"))\nrepeat = int(dbutils.widgets.get(\"repeat\"))\nlearning_rate = float(dbutils.widgets.get(\"learning_rate\"))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":7},{"cell_type":"code","source":["import time\nimport os\nimport datetime\nimport time\nimport pandas as pd \nimport numpy as np\n\ncheckpoint_dir = '/dbfs/ml/vgg16/train/{}/'.format(time.time())\n\nos.makedirs(checkpoint_dir)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":8},{"cell_type":"code","source":["### Removed BACKGROUND_Google folder \n### Why there are two faces folder - Faces and Faces_easy"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":9},{"cell_type":"code","source":["HEIGHT=224 #VGG16 224 VS InceptionV3 299\nWIDTH=224 #VGG16 224 VS InceptionV3 299\nTRAIN_DIR=\"/dbfs/tmp/caltech/train/\"  #need to add train\nTEST_DIR=\"/dbfs/tmp/caltech/test/\"   #need to add test\nNUM_CLASSES = 101 # number of category"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":10},{"cell_type":"code","source":["train_files = os.listdir(TRAIN_DIR)\ntotal_train_files = 0\ntrain_class = 0\nfor i in train_files:\n  i = TRAIN_DIR + '/' + i\n  if os.path.isdir(i):\n    total_train_files += len(os.listdir(i))\n    train_class += 1\n  \ntest_files = os.listdir(TEST_DIR)\ntotal_test_files = 0\ntest_class = 0\nfor i in test_files:\n  i = TEST_DIR + '/' + i\n  if os.path.isdir(i):\n    total_test_files += len(os.listdir(i))\n    test_class += 1\n\nprint(f\"Train - {train_class} classes contain {total_train_files} pictures\")\nprint(f\"Test - {test_class} classes contain {total_test_files} pictures\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Train - 101 classes contain 6982 pictures\nTest - 101 classes contain 1695 pictures\n</div>"]}}],"execution_count":11},{"cell_type":"code","source":["import matplotlib.pyplot as plt\nimport numpy as np\nfrom PIL import Image\nimport os\nimport time\n#https://github.com/CyberZHG/keras-radam/issues/12\nos.environ['TF_KERAS'] = '1' #apparently it doens't work, but %env tf_keras=1 works. everytime tried after clear state of the notebook"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":12},{"cell_type":"code","source":["%env TF_KERAS =1\n##https://github.com/CyberZHG/keras-radam/issues/12\n#https://github.com/CyberZHG/keras-lookahead/issues/1"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">env: TF_KERAS=1\n</div>"]}}],"execution_count":13},{"cell_type":"code","source":["#tf.keras\nfrom tensorflow import keras\nfrom tensorflow.keras import models\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Dense,Dropout,GlobalAveragePooling2D\nfrom tensorflow.keras.applications.vgg16 import VGG16\nfrom tensorflow.keras.applications.vgg16 import preprocess_input as vgg16_preprocess_input\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\nimport horovod.tensorflow.keras as hvd\nfrom tensorflow.keras import backend as K\nimport tensorflow as tf\n#np.set_printoptions(threshold=np.inf)\n\n#https://pypi.org/project/keras-rectified-adam/\nfrom keras_radam import RAdam"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">/databricks/python/lib/python3.7/site-packages/tensorflow/python/util/nest.py:1286: DeprecationWarning: Using or importing the ABCs from &#39;collections&#39; instead of from &#39;collections.abc&#39; is deprecated, and in 3.8 it will stop working\n  _pywrap_tensorflow.RegisterType(&#34;Mapping&#34;, _collections.Mapping)\nWARNING:tensorflow:From /databricks/python/lib/python3.7/site-packages/horovod/tensorflow/__init__.py:117: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n\nWARNING:tensorflow:From /databricks/python/lib/python3.7/site-packages/horovod/tensorflow/__init__.py:143: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n\n</div>"]}}],"execution_count":14},{"cell_type":"code","source":["### Try to write generator into pickle or other format but doesn't work, so each nodes need process this step separately\ndef get_dataset():\n\n  #https://stackoverflow.com/questions/44341258/preprocessing-function-of-inception-v3-in-keras\n  datagen = ImageDataGenerator(\n          rotation_range=180, # I really think unlike cats and dogs, pill image can be rotated 180 degrees\n          preprocessing_function=vgg16_preprocess_input,\n          width_shift_range=0.2,\n          height_shift_range=0.2,\n          #rescale=1./255, #note that prepocess_input will rescale, no need to do additional rescale. \n          shear_range=0.2,\n          zoom_range=0.2,\n          horizontal_flip=True,\n          fill_mode='nearest')\n\n  # Images will be directly taken form our defined folder structure using the method flow_from_directory()\n  train_generator =datagen.flow_from_directory(\n      TRAIN_DIR,\n      target_size=(HEIGHT, WIDTH),\n          batch_size=batch_size,\n          class_mode='categorical')\n\n  validation_generator = datagen.flow_from_directory(\n      TEST_DIR,\n      target_size=(HEIGHT, WIDTH),\n      batch_size=batch_size,\n      class_mode='categorical')\n  \n  return (train_generator,validation_generator)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":15},{"cell_type":"code","source":["#later need to change to get mdoel for multiple models.\n#https://www.programcreek.com/python/example/106229/keras.applications.inception_v3.InceptionV3\n\ndef get_model(num_classes = NUM_CLASSES):\n  base_model = VGG16(weights='imagenet',include_top=True) #top means the last 4 (flatten fc1, fc2, dense) layers. we only need to change dense layer. the rest are convolutional layers. \n  x = base_model.get_layer('fc2').output\n  predictions = Dense(NUM_CLASSES, activation='softmax')(x)\n  model = Model(inputs=base_model.input, outputs=predictions)\n  return model"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":16},{"cell_type":"code","source":["def train(learning_rate=learning_rate):\n  ### Get dataset \n  train_generator, validation_generator = get_dataset()\n  \n  ### Get model\n  model = get_model()\n\n  ### Model Compile \n  # optimizer = RAdam(total_steps=5000, warmup_proportion=0.1, min_lr=1e-5)\n  optimizer = keras.optimizers.Adadelta(lr=learning_rate)\n  \n  model.compile(optimizer = optimizer,#'rmsprop',\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\n  \n  history = model.fit_generator(\n      train_generator,\n      epochs=epochs,\n      validation_data=validation_generator,\n      verbose=2\n      #validation_steps=VALIDATION_STEPS,steps_per_epoch=STEPS_PER_EPOCH, \n  )\n  ### TO DO SAVE THE MODEL\n  # model.save(\"model_radam.h5\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":17},{"cell_type":"code","source":["### Define output log file name \nenv = \"TEST\"\ninstance_type = 'SINGLE_INSTANCE'\ndataset = 'CALTECH101'\nmodel_name = 'VGG16'\n\ntrain_shape = total_train_files\nvalidation_shape = total_test_files\n\ndef get_cluster_info(hvd_run=True):\n  driver_type = sc.getConf().get(\"spark.databricks.driverNodeTypeId\")\n  worker_type = \"None\"\n  num_workers = \"None\"\n  if hvd_run == True:\n    worker_type = sc.getConf().get(\"spark.databricks.workerNodeTypeId\")\n    num_workers = sc.getConf().get(\"spark.databricks.clusterUsageTags.clusterWorkers\")\n  return [driver_type, worker_type, num_workers]\n\ndriver_type, worker_type, num_workers = get_cluster_info()\nnp_setup = \"NA\""],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":18},{"cell_type":"code","source":["filename = f\"benchmark|{env}|{dataset}|{model_name}|{train_shape}|{validation_shape}|{instance_type}|{driver_type}|{worker_type}|{num_workers}|{np_setup}|{repeat}|{epochs}|{learning_rate}|{batch_size}.log\""],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":19},{"cell_type":"code","source":["# reset_stdout()\n# print(output_dir+filename)\n\n# redirect_stdout(output_dir+filename)\n# for i in range(repeat):\n#   print(f\"REPEAT {i+1}\")\n#   model = train()\n# move_log_to_s3()\n# reset_stdout()\n\n# ### batch_size 128 -> OOM\n\n# ### Lots of error closing message in logs: PIL.Image - DEBUG - Error closing: 'Image' object has no attribute 'fp'\n# ### Seems like no harm but might need check it tomorrow\n# ### https://github.com/romainbrette/holypipette/issues/80"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":20},{"cell_type":"code","source":["#fit_generator version of hvd\n#https://github.com/horovod/horovod/blob/master/examples/keras_mnist_advanced.py\n#btw, the advanced example used fancy adadelta optimizer, that's why there are so many call backs.\n#https://docs.databricks.com/applications/deep-learning/distributed-training/mnist-tensorflow-keras.html\n#my understanding is that the entire script of horovod exampe needs to be put in train_hvd fundtion\n\ndef train_hvd(learning_rate = learning_rate):\n  # Horovod: initialize Horovod.\n  hvd.init()\n\n  # Horovod: pin GPU to be used to process local rank (one GPU per process)\n  config = tf.ConfigProto()\n  config.gpu_options.allow_growth = True\n  config.gpu_options.visible_device_list = str(hvd.local_rank())\n  K.set_session(tf.Session(config=config))\n\n  \n  # Collect get data time\n  start_time = time.time()\n  \n  train_generator, validation_generator = get_dataset()\n  step_size_train = train_generator.n//train_generator.batch_size\n  step_size_validation = validation_generator.n//validation_generator.batch_size\n  \n  end_time = round((time.time() - start_time),3)\n  print(f\"step - get_data - {end_time}\")  \n\n  \n  # Model Compiles\n  start_time = time.time()\n\n  model=get_model()\n  # Horovod: adjust learning rate based on number of GPUs.\n  #optimizer = keras.optimizers.Adadelta(lr=learning_rate * hvd.size())\n  \n  optimizer = keras.optimizers.Adadelta(lr=learning_rate*hvd.size())\n  \n  # optimizer =RAdam(total_steps=5000, warmup_proportion=0.1, learning_rate=learning_rate*hvd.size(), min_lr=1e-5)\n\n  # Horovod: add Horovod Distributed Optimizer.\n  optimizer = hvd.DistributedOptimizer(optimizer)\n\n  model.compile(optimizer=optimizer,\n                loss='categorical_crossentropy',\n                metrics=['accuracy'])\n\n  callbacks = [\n      # Horovod: broadcast initial variable states from rank 0 to all other processes.\n      # This is necessary to ensure consistent initialization of all workers when\n      # training is started with random weights or restored from a checkpoint.\n      hvd.callbacks.BroadcastGlobalVariablesCallback(0),\n      hvd.callbacks.MetricAverageCallback(),\n      hvd.callbacks.LearningRateWarmupCallback(warmup_epochs=5, verbose=1),\n      keras.callbacks.ReduceLROnPlateau(patience=10, verbose=1)\n\n  ]\n\n  # Horovod: save checkpoints only on worker 0 to prevent other workers from corrupting them.\n  if hvd.rank() == 0:\n      callbacks.append(keras.callbacks.ModelCheckpoint(checkpoint_dir + 'checkpoint-{epoch}.ckpt', save_weights_only = True))\n\n  end_time = round((time.time() - start_time),3)\n  print(f\"step - prep_model - {end_time}\") \n  \n  # Model Train\n  start_time = time.time()\n  \n  history = model.fit_generator(\n    generator = train_generator,\n    steps_per_epoch = step_size_train // hvd.size() ,\n    validation_data = validation_generator,\n    validation_steps = step_size_validation // hvd.size() , ## Not to oversample dataset\n    epochs = epochs,\n    callbacks = callbacks,\n    verbose=2\n    )#validation_steps=VALIDATION_STEPS,steps_per_epoch=STEPS_PER_EPOCH, I don't use them either here not only because I have a fixed sample size, and also I don't have to over sample validation set. \n  \n  end_time = round((time.time() - start_time),3)\n  print(f\"step - train_model - {end_time}\")   \n    "],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":21},{"cell_type":"code","source":["reset_stdout()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":22},{"cell_type":"code","source":["from sparkdl import HorovodRunner\ninstance_type = \"HOROVOD_CLUSTER\""],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Using TensorFlow backend.\n</div>"]}}],"execution_count":23},{"cell_type":"code","source":["start_time = time.time()\n  \nreset_stdout()\nnp_setup = 4\nfilename = f\"benchmark|{env}|{dataset}|{model_name}|{train_shape}|{validation_shape}|{instance_type}|{driver_type}|{worker_type}|{num_workers}|{np_setup}|{repeat}|{epochs}|{learning_rate}|{batch_size}.log\"\nprint(output_dir+filename)\nredirect_stdout(output_dir+filename)\n\nfor i in range(repeat):\n  print(f\"{instance_type} - REPEAT {i+1}\")\n  hr = HorovodRunner(np = np_setup)\n  hr.run(train_hvd,  learning_rate=learning_rate)\n\nreset_stdout()\n\nmove_log_to_s3()\nend_time = round((time.time() - start_time),3)\nprint(f\"finshed in {end_time}\") "],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">logs/logs/horovod_logs/2019_12_11_16_00_36/benchmark|TEST|CALTECH101|VGG16|6982|1695|HOROVOD_CLUSTER|p2.xlarge|p2.xlarge|4|4|1|40|0.001|64.log\n</div>"]}}],"execution_count":24},{"cell_type":"code","source":["### For 8 p2.xlarge instance \nnp_list = [2, 4, 6, 8]"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"code","source":["for _np in np_list:\n  \n  start_time = time.time()\n  \n  reset_stdout()\n  np_setup = _np\n  filename = f\"benchmark|{env}|{dataset}|{model_name}|{train_shape}|{validation_shape}|{instance_type}|{driver_type}|{worker_type}|{num_workers}|{np_setup}|{repeat}|{epochs}|{learning_rate}|{batch_size}.log\"\n  print(output_dir+filename)\n  redirect_stdout(output_dir+filename)\n\n  for i in range(repeat):\n    print(f\"{instance_type} - REPEAT {i+1}\")\n    hr = HorovodRunner(np = np_setup)\n    hr.run(train_hvd,  learning_rate=learning_rate)\n  \n  reset_stdout()\n  \n  move_log_to_s3()\n  end_time = round((time.time() - start_time),3)\n  print(f\"finshed in {end_time}\") "],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"code","source":["import numpy as np\nnp_list = list(np.linspace(1,8*8, 10, dtype = int))\n# np_list.insert(2,12)\n# np_list.insert(1,5)\n# np_list.insert(1,3)\n# np_list.insert(0,-8)\nnp_list.pop(0)\nnp_list"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"code","source":["np_list = [-8, -4, 1, 3, 5]"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"code","source":["for _np in np_list:\n  \n  start_time = time.time()\n  \n  reset_stdout()\n  np_setup = _np\n  filename = f\"benchmark|{env}|{dataset}|{model_name}|{train_shape}|{validation_shape}|{instance_type}|{driver_type}|{worker_type}|{num_workers}|{np_setup}|{repeat}|{epochs}|{learning_rate}|{batch_size}.log\"\n  print(output_dir+filename)\n  redirect_stdout(output_dir+filename)\n\n  for i in range(repeat):\n    print(f\"{instance_type} - REPEAT {i+1}\")\n    hr = HorovodRunner(np = np_setup)\n    hr.run(train_hvd,  learning_rate=learning_rate)\n  \n  reset_stdout()\n  \n  move_log_to_s3()\n  end_time = round((time.time() - start_time),3)\n  print(f\"finshed in {end_time}\") "],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"code","source":["def move_log_to_s3():\n  ### Moving all the driver logs to mounted s3 bucket. \n  import glob\n  from distutils import dir_util \n  ### Shutile will not overwrit the file so use distutils instead\n\n  list_dir = glob.glob(output_dir)\n\n  for path in list_dir:\n    dir_name = os.path.basename(os.path.dirname(path))\n    dest = final_output_dir + dir_name\n    destination = dir_util.copy_tree(path, dest) "],"metadata":{},"outputs":[],"execution_count":30}],"metadata":{"name":"np4_VGG horovod benchmark_debug_RAdam","notebookId":18026},"nbformat":4,"nbformat_minor":0}
