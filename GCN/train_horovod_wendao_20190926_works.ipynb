{"cells":[{"cell_type":"code","source":["# #use data_team_concurrent iam\n# dbutils.fs.cp(\"s3://ehth-databricks-poc-test/ehealth/JingP/cora/cora.cites\",\"dbfs:/tmp/cora/cora.cites\")\n# dbutils.fs.cp(\"s3://ehth-databricks-poc-test/ehealth/JingP/cora/cora.content\",\"dbfs:/tmp/cora/cora.content\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":1},{"cell_type":"code","source":["# #then use horovod cluster\n# dbutils.fs.cp(\"dbfs:/tmp/cora/cora.cites\",\"file:/tmp/cora/cora.cites\")\n# dbutils.fs.cp(\"dbfs:/tmp/cora/cora.content\",\"file:/tmp/cora/cora.content\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":2},{"cell_type":"code","source":["%run ./utils"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["%run ./layers/graph"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["import os\nimport time\n\ncheckpoint_dir = '/dbfs/kegra_horovod/train/{}/'.format(time.time())\nos.makedirs(checkpoint_dir)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":5},{"cell_type":"code","source":["from __future__ import print_function\n\nfrom keras.layers import Input, Dropout\nfrom keras.models import Model\nfrom keras.optimizers import Adam\nfrom keras.regularizers import l2\n\n#databricks import functions in the library by %run\n#from kegra.layers.graph import GraphConvolution\n#from kegra.utils import *\n\nimport time\n\n# Horovod: Import the relevant submodule\nimport horovod.keras as hvd\nfrom keras import backend as K\nimport tensorflow as tf\nimport keras"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":6},{"cell_type":"code","source":["# Define parameters\nDATASET = 'cora'\nPATH='/dbfs/tmp/cora/'\nFILTER = 'localpool'  # 'chebyshev'\nMAX_DEGREE = 2  # maximum polynomial degree\nSYM_NORM = True  # symmetric (True) vs. left-only (False) normalization\nNB_EPOCH = 200\nPATIENCE = 10  # early stopping patience\nLEARNING_RATE=0.01"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":7},{"cell_type":"code","source":["%sh ls /tmp/cora"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">cora.cites\ncora.content\n</div>"]}}],"execution_count":8},{"cell_type":"code","source":["def train_hvd(learning_rate=LEARNING_RATE):\n  # Horovod: initialize Horovod.\n  hvd.init()\n  \n  # Get data\n  X, A, y = load_data(path=PATH,dataset=DATASET)\n  y_train, y_val, y_test, idx_train, idx_val, idx_test, train_mask = get_splits(y)\n\n  # Normalize X\n  X /= X.sum(1).reshape(-1, 1)\n  \n  if FILTER == 'localpool':\n    \"\"\" Local pooling filters (see 'renormalization trick' in Kipf & Welling, arXiv 2016) \"\"\"\n    print('Using local pooling filters...')\n    A_ = preprocess_adj(A, SYM_NORM)\n    support = 1\n    graph = [X, A_]\n    G = [Input(shape=(None, None), batch_shape=(None, None), sparse=True)]\n\n  elif FILTER == 'chebyshev':\n    \"\"\" Chebyshev polynomial basis filters (Defferard et al., NIPS 2016)  \"\"\"\n    print('Using Chebyshev polynomial basis filters...')\n    L = normalized_laplacian(A, SYM_NORM)\n    L_scaled = rescale_laplacian(L)\n    T_k = chebyshev_polynomial(L_scaled, MAX_DEGREE)\n    support = MAX_DEGREE + 1\n    graph = [X]+T_k\n    G = [Input(shape=(None, None), batch_shape=(None, None), sparse=True) for _ in range(support)]\n\n  else:\n      raise Exception('Invalid filter type.')\n\n  X_in = Input(shape=(X.shape[1],))\n  \n  print(\"X_in shape is {}\".format(X_in.shape))\n   \n  # Define model architecture\n  # NOTE: We pass arguments for graph convolutional layers as a list of tensors.\n  # This is somewhat hacky, more elegant options would require rewriting the Layer base class.\n  H = Dropout(0.5)(X_in)\n  H = GraphConvolution(16, support, activation='relu', kernel_regularizer=l2(5e-4))([H]+G)\n  H = Dropout(0.5)(H)\n  Y = GraphConvolution(y.shape[1], support, activation='softmax')([H]+G)\n\n  # Compile model\n  model = Model(inputs=[X_in]+G, outputs=Y)\n  model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=learning_rate))\n  \n  print (\"model compile is done\")\n  \n  callbacks = [\n      # Horovod: broadcast initial variable states from rank 0 to all other processes.\n      # This is necessary to ensure consistent initialization of all workers when\n      # training is started with random weights or restored from a checkpoint.\n      hvd.callbacks.BroadcastGlobalVariablesCallback(0),\n  ]\n\n  # Horovod: save checkpoints only on worker 0 to prevent other workers from corrupting them.\n  if hvd.rank() == 0:\n      callbacks.append(keras.callbacks.ModelCheckpoint(checkpoint_dir + '/checkpoint-{epoch}.ckpt', save_weights_only = True,period=100))\n      \n  \n  print (\"hvd call back done\")\n  \n  \n\n  # Helper variables for main training loop\n  wait = 0\n  preds = None\n  best_val_loss = 99999\n\n  # Fit\n\n  # Log wall-clock time\n \n  # Single training iteration (we mask nodes without labels for loss calculation)\n  model.fit(graph, y_train, sample_weight=train_mask,\\\n            batch_size=A.shape[0], epochs=NB_EPOCH, verbose=2, shuffle=False)\n        \n\nt = time.time()  \ntrain_hvd()\nprint (\"time cost is {}\".format(time.time()-t))\n#print(X_in.shape)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Loading cora dataset...\nDataset has 2708 nodes, 5429 edges, 1433 features.\nUsing local pooling filters...\nX_in shape is (?, 1433)\nmodel compile is done\nhvd call back done\nEpoch 1/200\n - 1s - loss: 1.9614\nEpoch 2/200\n - 0s - loss: 1.9490\nEpoch 3/200\n - 0s - loss: 1.9327\nEpoch 4/200\n - 0s - loss: 1.9191\nEpoch 5/200\n - 0s - loss: 1.9008\nEpoch 6/200\n - 0s - loss: 1.8816\nEpoch 7/200\n - 0s - loss: 1.8722\nEpoch 8/200\n - 0s - loss: 1.8580\nEpoch 9/200\n - 0s - loss: 1.8431\nEpoch 10/200\n - 0s - loss: 1.8265\nEpoch 11/200\n - 0s - loss: 1.8158\nEpoch 12/200\n - 0s - loss: 1.7890\nEpoch 13/200\n - 0s - loss: 1.7913\nEpoch 14/200\n - 0s - loss: 1.7919\nEpoch 15/200\n - 0s - loss: 1.7711\nEpoch 16/200\n - 0s - loss: 1.7483\nEpoch 17/200\n - 0s - loss: 1.7536\nEpoch 18/200\n - 0s - loss: 1.7612\nEpoch 19/200\n - 0s - loss: 1.7388\nEpoch 20/200\n - 0s - loss: 1.7350\nEpoch 21/200\n - 0s - loss: 1.7234\nEpoch 22/200\n - 0s - loss: 1.7332\nEpoch 23/200\n - 0s - loss: 1.7180\nEpoch 24/200\n - 0s - loss: 1.7018\nEpoch 25/200\n - 0s - loss: 1.7013\nEpoch 26/200\n - 0s - loss: 1.7035\nEpoch 27/200\n - 0s - loss: 1.6541\nEpoch 28/200\n - 0s - loss: 1.6566\nEpoch 29/200\n - 0s - loss: 1.6493\nEpoch 30/200\n - 0s - loss: 1.6619\nEpoch 31/200\n - 0s - loss: 1.6481\nEpoch 32/200\n - 0s - loss: 1.6393\nEpoch 33/200\n - 0s - loss: 1.6386\nEpoch 34/200\n - 0s - loss: 1.6066\nEpoch 35/200\n - 0s - loss: 1.6335\nEpoch 36/200\n - 0s - loss: 1.5966\nEpoch 37/200\n - 0s - loss: 1.5762\nEpoch 38/200\n - 0s - loss: 1.5958\nEpoch 39/200\n - 0s - loss: 1.5736\nEpoch 40/200\n - 0s - loss: 1.5382\nEpoch 41/200\n - 0s - loss: 1.5532\nEpoch 42/200\n - 0s - loss: 1.5571\nEpoch 43/200\n - 0s - loss: 1.5429\nEpoch 44/200\n - 0s - loss: 1.5419\nEpoch 45/200\n - 0s - loss: 1.5384\nEpoch 46/200\n - 0s - loss: 1.5132\nEpoch 47/200\n - 0s - loss: 1.4964\nEpoch 48/200\n - 0s - loss: 1.5111\nEpoch 49/200\n - 0s - loss: 1.5021\nEpoch 50/200\n - 0s - loss: 1.4796\nEpoch 51/200\n - 0s - loss: 1.4382\nEpoch 52/200\n - 0s - loss: 1.4127\nEpoch 53/200\n - 0s - loss: 1.4512\nEpoch 54/200\n - 0s - loss: 1.4428\nEpoch 55/200\n - 0s - loss: 1.4319\nEpoch 56/200\n - 0s - loss: 1.4416\nEpoch 57/200\n - 0s - loss: 1.4212\nEpoch 58/200\n - 0s - loss: 1.4067\nEpoch 59/200\n - 0s - loss: 1.3871\nEpoch 60/200\n - 0s - loss: 1.4097\nEpoch 61/200\n - 0s - loss: 1.3804\nEpoch 62/200\n - 0s - loss: 1.3982\nEpoch 63/200\n - 0s - loss: 1.3521\nEpoch 64/200\n - 0s - loss: 1.3961\nEpoch 65/200\n - 0s - loss: 1.3387\nEpoch 66/200\n - 0s - loss: 1.3338\nEpoch 67/200\n - 0s - loss: 1.3256\nEpoch 68/200\n - 0s - loss: 1.3609\nEpoch 69/200\n - 0s - loss: 1.3234\nEpoch 70/200\n - 0s - loss: 1.2976\nEpoch 71/200\n - 0s - loss: 1.3421\nEpoch 72/200\n - 0s - loss: 1.3315\nEpoch 73/200\n - 0s - loss: 1.3029\nEpoch 74/200\n - 0s - loss: 1.2673\nEpoch 75/200\n - 0s - loss: 1.2915\nEpoch 76/200\n - 0s - loss: 1.2600\nEpoch 77/200\n - 0s - loss: 1.2933\nEpoch 78/200\n - 0s - loss: 1.2634\nEpoch 79/200\n - 0s - loss: 1.2574\nEpoch 80/200\n - 0s - loss: 1.2289\nEpoch 81/200\n - 0s - loss: 1.2692\nEpoch 82/200\n - 0s - loss: 1.2301\nEpoch 83/200\n - 0s - loss: 1.2115\nEpoch 84/200\n - 0s - loss: 1.2131\nEpoch 85/200\n - 0s - loss: 1.1902\nEpoch 86/200\n - 0s - loss: 1.2002\nEpoch 87/200\n - 0s - loss: 1.2063\nEpoch 88/200\n - 0s - loss: 1.1956\nEpoch 89/200\n - 0s - loss: 1.1743\nEpoch 90/200\n - 0s - loss: 1.2278\nEpoch 91/200\n - 0s - loss: 1.1463\nEpoch 92/200\n - 0s - loss: 1.2096\nEpoch 93/200\n - 0s - loss: 1.1334\nEpoch 94/200\n - 0s - loss: 1.1577\nEpoch 95/200\n - 0s - loss: 1.1360\nEpoch 96/200\n - 0s - loss: 1.1596\nEpoch 97/200\n - 0s - loss: 1.1449\nEpoch 98/200\n - 0s - loss: 1.0899\nEpoch 99/200\n - 0s - loss: 1.1565\nEpoch 100/200\n - 0s - loss: 1.1353\nEpoch 101/200\n - 0s - loss: 1.0973\nEpoch 102/200\n - 0s - loss: 1.1112\nEpoch 103/200\n - 0s - loss: 1.0983\nEpoch 104/200\n - 0s - loss: 1.1623\nEpoch 105/200\n - 0s - loss: 1.1375\nEpoch 106/200\n - 0s - loss: 1.0988\nEpoch 107/200\n - 0s - loss: 1.1168\nEpoch 108/200\n - 0s - loss: 1.1126\nEpoch 109/200\n - 0s - loss: 1.1307\nEpoch 110/200\n - 0s - loss: 1.0392\nEpoch 111/200\n - 0s - loss: 1.1252\nEpoch 112/200\n - 0s - loss: 1.0721\nEpoch 113/200\n - 0s - loss: 1.0864\nEpoch 114/200\n - 0s - loss: 1.0801\nEpoch 115/200\n - 0s - loss: 1.0813\nEpoch 116/200\n - 0s - loss: 1.0152\nEpoch 117/200\n - 0s - loss: 1.0691\nEpoch 118/200\n - 0s - loss: 1.0681\nEpoch 119/200\n - 0s - loss: 1.0264\nEpoch 120/200\n - 0s - loss: 1.0078\nEpoch 121/200\n - 0s - loss: 1.0529\nEpoch 122/200\n - 0s - loss: 1.0135\nEpoch 123/200\n - 0s - loss: 1.0152\nEpoch 124/200\n - 0s - loss: 1.0148\nEpoch 125/200\n - 0s - loss: 0.9914\nEpoch 126/200\n - 0s - loss: 0.9866\nEpoch 127/200\n - 0s - loss: 1.0389\nEpoch 128/200\n - 0s - loss: 1.0306\nEpoch 129/200\n - 0s - loss: 0.9930\nEpoch 130/200\n - 0s - loss: 0.9954\nEpoch 131/200\n - 0s - loss: 0.9857\nEpoch 132/200\n - 0s - loss: 1.0039\nEpoch 133/200\n - 0s - loss: 0.9928\nEpoch 134/200\n - 0s - loss: 1.0127\nEpoch 135/200\n - 0s - loss: 1.0043\nEpoch 136/200\n - 0s - loss: 0.9897\nEpoch 137/200\n - 0s - loss: 0.9917\nEpoch 138/200\n - 0s - loss: 1.0116\nEpoch 139/200\n - 0s - loss: 0.9791\nEpoch 140/200\n - 0s - loss: 0.9703\nEpoch 141/200\n - 0s - loss: 0.9373\nEpoch 142/200\n - 0s - loss: 0.9561\nEpoch 143/200\n - 0s - loss: 0.9473\nEpoch 144/200\n - 0s - loss: 0.9881\nEpoch 145/200\n - 0s - loss: 1.0057\nEpoch 146/200\n - 0s - loss: 0.9327\nEpoch 147/200\n - 0s - loss: 1.0135\nEpoch 148/200\n - 0s - loss: 0.9594\nEpoch 149/200\n - 0s - loss: 0.9244\nEpoch 150/200\n - 0s - loss: 0.9554\nEpoch 151/200\n - 0s - loss: 0.9767\nEpoch 152/200\n - 0s - loss: 0.9309\nEpoch 153/200\n - 0s - loss: 0.8908\nEpoch 154/200\n - 0s - loss: 0.9302\nEpoch 155/200\n - 0s - loss: 0.8922\nEpoch 156/200\n - 0s - loss: 0.9788\nEpoch 157/200\n - 0s - loss: 0.8999\nEpoch 158/200\n - 0s - loss: 0.9679\nEpoch 159/200\n - 0s - loss: 0.9120\nEpoch 160/200\n - 0s - loss: 0.9311\nEpoch 161/200\n - 0s - loss: 0.8989\nEpoch 162/200\n - 0s - loss: 0.9276\nEpoch 163/200\n - 0s - loss: 0.9110\nEpoch 164/200\n - 0s - loss: 0.9465\nEpoch 165/200\n - 0s - loss: 0.8897\nEpoch 166/200\n - 0s - loss: 0.8683\nEpoch 167/200\n - 0s - loss: 0.9307\nEpoch 168/200\n - 0s - loss: 0.8936\nEpoch 169/200\n - 0s - loss: 0.8689\nEpoch 170/200\n - 0s - loss: 0.9249\nEpoch 171/200\n - 0s - loss: 0.8947\nEpoch 172/200\n - 0s - loss: 0.9074\nEpoch 173/200\n - 0s - loss: 0.9432\nEpoch 174/200\n - 0s - loss: 0.8882\nEpoch 175/200\n - 0s - loss: 0.9066\nEpoch 176/200\n - 0s - loss: 0.9125\nEpoch 177/200\n - 0s - loss: 0.8891\nEpoch 178/200\n - 0s - loss: 0.8968\nEpoch 179/200\n - 0s - loss: 0.8864\nEpoch 180/200\n - 0s - loss: 0.8646\nEpoch 181/200\n - 0s - loss: 0.8360\nEpoch 182/200\n - 0s - loss: 0.9272\nEpoch 183/200\n - 0s - loss: 0.8597\nEpoch 184/200\n - 0s - loss: 0.8803\nEpoch 185/200\n - 0s - loss: 0.8760\nEpoch 186/200\n - 0s - loss: 0.8951\nEpoch 187/200\n - 0s - loss: 0.8885\nEpoch 188/200\n - 0s - loss: 0.8697\nEpoch 189/200\n - 0s - loss: 0.9000\nEpoch 190/200\n - 0s - loss: 0.8693\nEpoch 191/200\n - 0s - loss: 0.8606\nEpoch 192/200\n - 0s - loss: 0.8279\nEpoch 193/200\n - 0s - loss: 0.8541\nEpoch 194/200\n - 0s - loss: 0.8518\nEpoch 195/200\n - 0s - loss: 0.8378\nEpoch 196/200\n - 0s - loss: 0.8237\nEpoch 197/200\n - 0s - loss: 0.8225\nEpoch 198/200\n - 0s - loss: 0.8182\nEpoch 199/200\n - 0s - loss: 0.8479\nEpoch 200/200\n - 0s - loss: 0.7975\ntime cost is 19.16066288948059\n</div>"]}}],"execution_count":9},{"cell_type":"code","source":["from sparkdl import HorovodRunner\nt = time.time()\nhr = HorovodRunner(np=8)\nmodel = hr.run(train_hvd, learning_rate=LEARNING_RATE)\nprint (\"time cost is {}\".format(time.time()-t))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">The global names read or written to by the pickled function are {&#39;PATH&#39;, &#39;Adam&#39;, &#39;SYM_NORM&#39;, &#39;keras&#39;, &#39;FILTER&#39;, &#39;print&#39;, &#39;Dropout&#39;, &#39;hvd&#39;, &#39;Input&#39;, &#39;GraphConvolution&#39;, &#39;DATASET&#39;, &#39;normalized_laplacian&#39;, &#39;chebyshev_polynomial&#39;, &#39;get_splits&#39;, &#39;NB_EPOCH&#39;, &#39;Exception&#39;, &#39;Model&#39;, &#39;preprocess_adj&#39;, &#39;rescale_laplacian&#39;, &#39;checkpoint_dir&#39;, &#39;range&#39;, &#39;l2&#39;, &#39;MAX_DEGREE&#39;, &#39;load_data&#39;}.\nThe pickled object size is 10163 bytes.\n\n### How to enable Horovod Timeline? ###\nHorovodRunner has the ability to record the timeline of its activity with Horovod  Timeline. To\nrecord a Horovod Timeline, set the `HOROVOD_TIMELINE` environment variable  to the location of the\ntimeline file to be created. You can then open the timeline file  using the chrome://tracing\nfacility of the Chrome browser.\n\nStart training.\n/usr/bin/ssh: /databricks/python/lib/libcrypto.so.1.0.0: no version information available (required by /usr/bin/ssh)\n/usr/bin/ssh: /databricks/python/lib/libcrypto.so.1.0.0: no version information available (required by /usr/bin/ssh)\n/usr/bin/ssh: /databricks/python/lib/libcrypto.so.1.0.0: no version information available (required by /usr/bin/ssh)\n/usr/bin/ssh: /databricks/python/lib/libcrypto.so.1.0.0: no version information available (required by /usr/bin/ssh)\n/usr/bin/ssh: /databricks/python/lib/libcrypto.so.1.0.0: no version information available (required by /usr/bin/ssh)\n/usr/bin/ssh: /databricks/python/lib/libcrypto.so.1.0.0: no version information available (required by /usr/bin/ssh)\n/usr/bin/ssh: /databricks/python/lib/libcrypto.so.1.0.0: no version information available (required by /usr/bin/ssh)\n/usr/bin/ssh: /databricks/python/lib/libcrypto.so.1.0.0: no version information available (required by /usr/bin/ssh)\n/usr/bin/ssh: /databricks/python/lib/libcrypto.so.1.0.0: no version information available (required by /usr/bin/ssh)\n/usr/bin/ssh: /databricks/python/lib/libcrypto.so.1.0.0: no version information available (required by /usr/bin/ssh)\n/usr/bin/ssh: /databricks/python/lib/libcrypto.so.1.0.0: no version information available (required by /usr/bin/ssh)\n/usr/bin/ssh: /databricks/python/lib/libcrypto.so.1.0.0: no version information available (required by /usr/bin/ssh)\n/usr/bin/ssh: /databricks/python/lib/libcrypto.so.1.0.0: no version information available (required by /usr/bin/ssh)\n/usr/bin/ssh: /databricks/python/lib/libcrypto.so.1.0.0: no version information available (required by /usr/bin/ssh)\n[1,0]&lt;stderr&gt;:Using TensorFlow backend.\n[1,3]&lt;stdout&gt;:Loading cora dataset...\n[1,6]&lt;stdout&gt;:Loading cora dataset...\n[1,2]&lt;stdout&gt;:Loading cora dataset...\n[1,1]&lt;stdout&gt;:Loading cora dataset...\n[1,5]&lt;stdout&gt;:Loading cora dataset...\n[1,4]&lt;stdout&gt;:Loading cora dataset...\n[1,7]&lt;stdout&gt;:Loading cora dataset...\n[1,0]&lt;stdout&gt;:Loading cora dataset...\n[1,7]&lt;stdout&gt;:Dataset has 2708 nodes, 5429 edges, 1433 features.\n[1,7]&lt;stderr&gt;:OMP: Info #209: KMP_AFFINITY: decoding x2APIC ids.\n[1,7]&lt;stderr&gt;:OMP: Info #207: KMP_AFFINITY: Affinity capable, using global cpuid leaf 11 info\n[1,7]&lt;stderr&gt;:OMP: Info #154: KMP_AFFINITY: Initial OS proc set respected: {0,1,2,3,4,5,6,7}\n[1,7]&lt;stderr&gt;:OMP: Info #156: KMP_AFFINITY: 8 available OS procs\n[1,7]&lt;stderr&gt;:OMP: Info #157: KMP_AFFINITY: Uniform topology\n[1,7]&lt;stderr&gt;:OMP: Info #179: KMP_AFFINITY: 1 packages x 4 cores/pkg x 2 threads/core (4 total cores)\n[1,7]&lt;stderr&gt;:OMP: Info #211: KMP_AFFINITY: OS proc to physical thread map:\n[1,7]&lt;stderr&gt;:OMP: Info #171: KMP_AFFINITY: OS proc 0 maps to package 0 core 0 thread 0 \n[1,7]&lt;stderr&gt;:OMP: Info #171: KMP_AFFINITY: OS proc 4 maps to package 0 core 0 thread 1 \n[1,7]&lt;stderr&gt;:OMP: Info #171: KMP_AFFINITY: OS proc 1 maps to package 0 core 1 thread 0 \n[1,7]&lt;stderr&gt;:OMP: Info #171: KMP_AFFINITY: OS proc 5 maps to package 0 core 1 thread 1 \n[1,7]&lt;stderr&gt;:OMP: Info #171: KMP_AFFINITY: OS proc 2 maps to package 0 core 2 thread 0 \n[1,7]&lt;stderr&gt;:OMP: Info #171: KMP_AFFINITY: OS proc 6 maps to package 0 core 2 thread 1 \n[1,7]&lt;stderr&gt;:OMP: Info #171: KMP_AFFINITY: OS proc 3 maps to package 0 core 3 thread 0 \n[1,7]&lt;stderr&gt;:OMP: Info #171: KMP_AFFINITY: OS proc 7 maps to package 0 core 3 thread 1 \n[1,7]&lt;stderr&gt;:OMP: Info #247: KMP_AFFINITY: pid 3324 tid 3324 thread 0 bound to OS proc set {0}\n[1,3]&lt;stdout&gt;:Dataset has 2708 nodes, 5429 edges, 1433 features.\n[1,7]&lt;stdout&gt;:Using local pooling filters...\n[1,7]&lt;stdout&gt;:X_in shape is (?, 1433)\n[1,3]&lt;stderr&gt;:OMP: Info #209: KMP_AFFINITY: decoding x2APIC ids.\n[1,3]&lt;stderr&gt;:OMP: Info #207: KMP_AFFINITY: Affinity capable, using global cpuid leaf 11 info\n[1,3]&lt;stderr&gt;:OMP: Info #154: KMP_AFFINITY: Initial OS proc set respected: {0,1,2,3,4,5,6,7}\n[1,3]&lt;stderr&gt;:OMP: Info #156: KMP_AFFINITY: 8 available OS procs\n[1,3]&lt;stderr&gt;:OMP: Info #157: KMP_AFFINITY: Uniform topology\n[1,3]&lt;stderr&gt;:OMP: Info #179: KMP_AFFINITY: 1 packages x 4 cores/pkg x 2 threads/core (4 total cores)\n[1,3]&lt;stderr&gt;:OMP: Info #211: KMP_AFFINITY: OS proc to physical thread map:\n[1,3]&lt;stderr&gt;:OMP: Info #171: KMP_AFFINITY: OS proc 0 maps to package 0 core 0 thread 0 \n[1,3]&lt;stderr&gt;:OMP: Info #171: KMP_AFFINITY: OS proc 4 maps to package 0 core 0 thread 1 \n[1,3]&lt;stderr&gt;:OMP: Info #171: KMP_AFFINITY: OS proc 1 maps to package 0 core 1 thread 0 \n[1,3]&lt;stderr&gt;:OMP: Info #171: KMP_AFFINITY: OS proc 5 maps to package 0 core 1 thread 1 \n[1,3]&lt;stderr&gt;:OMP: Info #171: KMP_AFFINITY: OS proc 2 maps to package 0 core 2 thread 0 \n[1,3]&lt;stderr&gt;:OMP: Info #171: KMP_AFFINITY: OS proc 6 maps to package 0 core 2 thread 1 \n[1,3]&lt;stderr&gt;:OMP: Info #171: KMP_AFFINITY: OS proc 3 maps to package 0 core 3 thread 0 \n[1,3]&lt;stderr&gt;:OMP: Info #171: KMP_AFFINITY: OS proc 7 maps to package 0 core 3 thread 1 \n[1,3]&lt;stderr&gt;:OMP: Info #247: KMP_AFFINITY: pid 3195 tid 3195 thread 0 bound to OS proc set {0}\n[1,7]&lt;stderr&gt;:Using TensorFlow backend.\n[1,7]&lt;stderr&gt;:WARNING:tensorflow:From /databricks/python/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py:423: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n[1,7]&lt;stderr&gt;:Instructions for updating:\n[1,7]&lt;stderr&gt;:Colocations handled automatically by placer.\n[1,7]&lt;stderr&gt;:From /databricks/python/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py:423: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n[1,7]&lt;stderr&gt;:Instructions for updating:\n[1,7]&lt;stderr&gt;:Colocations handled automatically by placer.\n[1,1]&lt;stdout&gt;:Dataset has 2708 nodes, 5429 edges, 1433 features.\n[1,7]&lt;stderr&gt;:WARNING:tensorflow:From /databricks/python/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n[1,7]&lt;stderr&gt;:Instructions for updating:\n[1,7]&lt;stderr&gt;:Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n[1,7]&lt;stderr&gt;:From /databricks/python/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n[1,7]&lt;stderr&gt;:Instructions for updating:\n[1,7]&lt;stderr&gt;:Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n[1,4]&lt;stdout&gt;:Dataset has 2708 nodes, 5429 edges, 1433 features.\n[1,3]&lt;stdout&gt;:Using local pooling filters...\n[1,1]&lt;stderr&gt;:OMP: Info #209: KMP_AFFINITY: decoding x2APIC ids.\n[1,1]&lt;stderr&gt;:OMP: Info #207: KMP_AFFINITY: Affinity capable, using global cpuid leaf 11 info\n[1,1]&lt;stderr&gt;:OMP: Info #154: KMP_AFFINITY: Initial OS proc set respected: {0,1,2,3,4,5,6,7}\n[1,1]&lt;stderr&gt;:OMP: Info #156: KMP_AFFINITY: 8 available OS procs\n[1,1]&lt;stderr&gt;:OMP: Info #157: KMP_AFFINITY: Uniform topology\n[1,1]&lt;stderr&gt;:OMP: Info #179: KMP_AFFINITY: 1 packages x 4 cores/pkg x 2 threads/core (4 total cores)\n[1,1]&lt;stderr&gt;:OMP: Info #211: KMP_AFFINITY: OS proc to physical thread map:\n[1,1]&lt;stderr&gt;:OMP: Info #171: KMP_AFFINITY: OS proc 0 maps to package 0 core 0 thread 0 \n[1,1]&lt;stderr&gt;:OMP: Info #171: KMP_AFFINITY: OS proc 4 maps to package 0 core 0 thread 1 \n[1,1]&lt;stderr&gt;:OMP: Info #171: KMP_AFFINITY: OS proc 1 maps to package 0 core 1 thread 0 \n[1,1]&lt;stderr&gt;:OMP: Info #171: KMP_AFFINITY: OS proc 5 maps to package 0 core 1 thread 1 \n[1,1]&lt;stderr&gt;:OMP: Info #171: KMP_AFFINITY: OS proc 2 maps to package 0 core 2 thread 0 \n[1,1]&lt;stderr&gt;:OMP: Info #171: KMP_AFFINITY: OS proc 6 maps to package 0 core 2 thread 1 \n[1,1]&lt;stderr&gt;:OMP: Info #171: KMP_AFFINITY: OS proc 3 maps to package 0 core 3 thread 0 \n[1,3]&lt;stdout&gt;:X_in shape is (?, 1433)\n[1,1]&lt;stderr&gt;:OMP: Info #171: KMP_AFFINITY: OS proc 7 maps to package 0 core 3 thread 1 \n[1,1]&lt;stderr&gt;:OMP: Info #247: KMP_AFFINITY: pid 3242 tid 3242 thread 0 bound to OS proc set {0}\n[1,2]&lt;stdout&gt;:Dataset has 2708 nodes, 5429 edges, 1433 features.\n[1,4]&lt;stderr&gt;:OMP: Info #209: KMP_AFFINITY: decoding x2APIC ids.\n[1,4]&lt;stderr&gt;:OMP: Info #207: KMP_AFFINITY: Affinity capable, using global cpuid leaf 11 info\n[1,4]&lt;stderr&gt;:OMP: Info #154: KMP_AFFINITY: Initial OS proc set respected: {0,1,2,3,4,5,6,7}\n[1,4]&lt;stderr&gt;:OMP: Info #156: KMP_AFFINITY: 8 available OS procs\n[1,4]&lt;stderr&gt;:OMP: Info #157: KMP_AFFINITY: Uniform topology\n[1,4]&lt;stderr&gt;:OMP: Info #179: KMP_AFFINITY: 1 packages x 4 cores/pkg x 2 threads/core (4 total cores)\n[1,4]&lt;stderr&gt;:OMP: Info #211: KMP_AFFINITY: OS proc to physical thread map:\n[1,4]&lt;stderr&gt;:OMP: Info #171: KMP_AFFINITY: OS proc 0 maps to package 0 core 0 thread 0 \n[1,4]&lt;stderr&gt;:OMP: Info #171: KMP_AFFINITY: OS proc 4 maps to package 0 core 0 thread 1 \n[1,4]&lt;stderr&gt;:OMP: Info #171: KMP_AFFINITY: OS proc 1 maps to package 0 core 1 thread 0 \n[1,4]&lt;stderr&gt;:OMP: Info #171: KMP_AFFINITY: OS proc 5 maps to package 0 core 1 thread 1 \n[1,4]&lt;stderr&gt;:OMP: Info #171: KMP_AFFINITY: OS proc 2 maps to package 0 core 2 thread 0 \n[1,4]&lt;stderr&gt;:OMP: Info #171: KMP_AFFINITY: OS proc 6 maps to package 0 core 2 thread 1 \n[1,4]&lt;stderr&gt;:OMP: Info #171: KMP_AFFINITY: OS proc 3 maps to package 0 core 3 thread 0 \n[1,4]&lt;stderr&gt;:OMP: Info #171: KMP_AFFINITY: OS proc 7 maps to package 0 core 3 thread 1 \n[1,4]&lt;stderr&gt;:OMP: Info #247: KMP_AFFINITY: pid 3212 tid 3212 thread 0 bound to OS proc set {0}\n[1,3]&lt;stderr&gt;:Using TensorFlow backend.\n[1,3]&lt;stderr&gt;:WARNING:tensorflow:From /databricks/python/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py:423: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n[1,3]&lt;stderr&gt;:Instructions for updating:\n[1,3]&lt;stderr&gt;:Colocations handled automatically by placer.\n[1,3]&lt;stderr&gt;:From /databricks/python/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py:423: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n[1,3]&lt;stderr&gt;:Instructions for updating:\n[1,3]&lt;stderr&gt;:Colocations handled automatically by placer.\n[1,3]&lt;stderr&gt;:WARNING:tensorflow:From /databricks/python/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n[1,3]&lt;stderr&gt;:Instructions for updating:\n[1,3]&lt;stderr&gt;:Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n[1,3]&lt;stderr&gt;:From /databricks/python/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n[1,3]&lt;stderr&gt;:Instructions for updating:\n[1,3]&lt;stderr&gt;:Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n[1,1]&lt;stdout&gt;:Using local pooling filters...\n[1,4]&lt;stdout&gt;:Using local pooling filters...\n[1,1]&lt;stdout&gt;:X_in shape is (?, 1433)\n[1,2]&lt;stderr&gt;:OMP: Info #209: KMP_AFFINITY: decoding x2APIC ids.\n[1,4]&lt;stdout&gt;:X_in shape is (?, 1433)\n[1,1]&lt;stderr&gt;:Using TensorFlow backend.\n[1,1]&lt;stderr&gt;:WARNING:tensorflow:From /databricks/python/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py:423: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n[1,1]&lt;stderr&gt;:Instructions for updating:\n[1,1]&lt;stderr&gt;:Colocations handled automatically by placer.\n[1,1]&lt;stderr&gt;:From /databricks/python/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py:423: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n[1,1]&lt;stderr&gt;:Instructions for updating:\n[1,1]&lt;stderr&gt;:Colocations handled automatically by placer.\n[1,2]&lt;stderr&gt;:OMP: Info #207: KMP_AFFINITY: Affinity capable, using global cpuid leaf 11 info\n[1,2]&lt;stderr&gt;:OMP: Info #154: KMP_AFFINITY: Initial OS proc set respected: {0,1,2,3,4,5,6,7}\n[1,2]&lt;stderr&gt;:OMP: Info #156: KMP_AFFINITY: 8 available OS procs\n[1,2]&lt;stderr&gt;:OMP: Info #157: KMP_AFFINITY: Uniform topology\n[1,2]&lt;stderr&gt;:OMP: Info #179: KMP_AFFINITY: 1 packages x 4 cores/pkg x 2 threads/core (4 total cores)\n[1,2]&lt;stderr&gt;:OMP: Info #211: KMP_AFFINITY: OS proc to physical thread map:\n[1,2]&lt;stderr&gt;:OMP: Info #171: KMP_AFFINITY: OS proc 0 maps to package 0 core 0 thread 0 \n[1,2]&lt;stderr&gt;:OMP: Info #171: KMP_AFFINITY: OS proc 4 maps to package 0 core 0 thread 1 \n[1,2]&lt;stderr&gt;:OMP: Info #171: KMP_AFFINITY: OS proc 1 maps to package 0 core 1 thread 0 \n[1,2]&lt;stderr&gt;:OMP: Info #171: KMP_AFFINITY: OS proc 5 maps to package 0 core 1 thread 1 \n[1,2]&lt;stderr&gt;:OMP: Info #171: KMP_AFFINITY: OS proc 2 maps to package 0 core 2 thread 0 \n[1,2]&lt;stderr&gt;:OMP: Info #171: KMP_AFFINITY: OS proc 6 maps to package 0 core 2 thread 1 \n[1,2]&lt;stderr&gt;:OMP: Info #171: KMP_AFFINITY: OS proc 3 maps to package 0 core 3 thread 0 \n[1,2]&lt;stderr&gt;:OMP: Info #171: KMP_AFFINITY: OS proc 7 maps to package 0 core 3 thread 1 \n[1,2]&lt;stderr&gt;:OMP: Info #247: KMP_AFFINITY: pid 3223 tid 3223 thread 0 bound to OS proc set {0}\n[1,1]&lt;stderr&gt;:WARNING:tensorflow:From /databricks/python/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n[1,1]&lt;stderr&gt;:Instructions for updating:\n[1,1]&lt;stderr&gt;:Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n[1,1]&lt;stderr&gt;:From /databricks/python/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n[1,1]&lt;stderr&gt;:Instructions for updating:\n[1,1]&lt;stderr&gt;:Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n[1,4]&lt;stderr&gt;:Using TensorFlow backend.\n[1,4]&lt;stderr&gt;:WARNING:tensorflow:From /databricks/python/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py:423: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n[1,4]&lt;stderr&gt;:Instructions for updating:\n[1,4]&lt;stderr&gt;:Colocations handled automatically by placer.\n[1,4]&lt;stderr&gt;:From /databricks/python/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py:423: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n[1,4]&lt;stderr&gt;:Instructions for updating:\n[1,4]&lt;stderr&gt;:Colocations handled automatically by placer.\n[1,2]&lt;stdout&gt;:Using local pooling filters...\n[1,4]&lt;stderr&gt;:WARNING:tensorflow:From /databricks/python/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n[1,4]&lt;stderr&gt;:Instructions for updating:\n[1,4]&lt;stderr&gt;:Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n[1,4]&lt;stderr&gt;:From /databricks/python/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n[1,4]&lt;stderr&gt;:Instructions for updating:\n[1,4]&lt;stderr&gt;:Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n[1,2]&lt;stdout&gt;:X_in shape is (?, 1433)\n[1,2]&lt;stderr&gt;:Using TensorFlow backend.\n[1,2]&lt;stderr&gt;:WARNING:tensorflow:From /databricks/python/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py:423: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n[1,2]&lt;stderr&gt;:Instructions for updating:\n[1,2]&lt;stderr&gt;:Colocations handled automatically by placer.\n[1,2]&lt;stderr&gt;:From /databricks/python/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py:423: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n[1,2]&lt;stderr&gt;:Instructions for updating:\n[1,2]&lt;stderr&gt;:Colocations handled automatically by placer.\n[1,2]&lt;stderr&gt;:WARNING:tensorflow:From /databricks/python/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n[1,2]&lt;stderr&gt;:Instructions for updating:\n[1,2]&lt;stderr&gt;:Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n[1,2]&lt;stderr&gt;:From /databricks/python/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n[1,2]&lt;stderr&gt;:Instructions for updating:\n[1,2]&lt;stderr&gt;:Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n[1,5]&lt;stdout&gt;:Dataset has 2708 nodes, 5429 edges, 1433 features.\n[1,7]&lt;stdout&gt;:model compile is done\n[1,7]&lt;stdout&gt;:hvd call back done\n[1,5]&lt;stderr&gt;:OMP: Info #209: KMP_AFFINITY: decoding x2APIC ids.\n[1,5]&lt;stderr&gt;:OMP: Info #207: KMP_AFFINITY: Affinity capable, using global cpuid leaf 11 info\n[1,5]&lt;stderr&gt;:OMP: Info #154: KMP_AFFINITY: Initial OS proc set respected: {0,1,2,3,4,5,6,7}\n[1,5]&lt;stderr&gt;:OMP: Info #156: KMP_AFFINITY: 8 available OS procs\n[1,5]&lt;stderr&gt;:OMP: Info #157: KMP_AFFINITY: Uniform topology\n[1,5]&lt;stderr&gt;:OMP: Info #179: KMP_AFFINITY: 1 packages x 4 cores/pkg x 2 threads/core (4 total cores)\n[1,5]&lt;stderr&gt;:OMP: Info #211: KMP_AFFINITY: OS proc to physical thread map:\n[1,5]&lt;stderr&gt;:OMP: Info #171: KMP_AFFINITY: OS proc 0 maps to package 0 core 0 thread 0 \n[1,5]&lt;stderr&gt;:OMP: Info #171: KMP_AFFINITY: OS proc 4 maps to package 0 core 0 thread 1 \n[1,5]&lt;stderr&gt;:OMP: Info #171: KMP_AFFINITY: OS proc 1 maps to package 0 core 1 thread 0 \n[1,5]&lt;stderr&gt;:OMP: Info #171: KMP_AFFINITY: OS proc 5 maps to package 0 core 1 thread 1 \n[1,5]&lt;stderr&gt;:OMP: Info #171: KMP_AFFINITY: OS proc 2 maps to package 0 core 2 thread 0 \n[1,5]&lt;stderr&gt;:OMP: Info #171: KMP_AFFINITY: OS proc 6 maps to package 0 core 2 thread 1 \n[1,5]&lt;stderr&gt;:OMP: Info #171: KMP_AFFINITY: OS proc 3 maps to package 0 core 3 thread 0 \n[1,5]&lt;stderr&gt;:OMP: Info #171: KMP_AFFINITY: OS proc 7 maps to package 0 core 3 thread 1 \n[1,5]&lt;stderr&gt;:OMP: Info #247: KMP_AFFINITY: pid 3163 tid 3163 thread 0 bound to OS proc set {0}\n[1,5]&lt;stdout&gt;:Using local pooling filters...\n[1,5]&lt;stdout&gt;:X_in shape is (?, 1433)\n[1,3]&lt;stdout&gt;:model compile is done\n[1,3]&lt;stdout&gt;:hvd call back done\n[1,5]&lt;stderr&gt;:Using TensorFlow backend.\n[1,5]&lt;stderr&gt;:WARNING:tensorflow:From /databricks/python/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py:423: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n[1,5]&lt;stderr&gt;:Instructions for updating:\n[1,5]&lt;stderr&gt;:Colocations handled automatically by placer.\n[1,5]&lt;stderr&gt;:From /databricks/python/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py:423: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n[1,5]&lt;stderr&gt;:Instructions for updating:\n[1,5]&lt;stderr&gt;:Colocations handled automatically by placer.\n[1,5]&lt;stderr&gt;:WARNING:tensorflow:From /databricks/python/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n[1,5]&lt;stderr&gt;:Instructions for updating:\n[1,5]&lt;stderr&gt;:Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n[1,5]&lt;stderr&gt;:From /databricks/python/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n[1,5]&lt;stderr&gt;:Instructions for updating:\n[1,5]&lt;stderr&gt;:Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n[1,7]&lt;stderr&gt;:WARNING:tensorflow:From /databricks/python/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n[1,7]&lt;stderr&gt;:Instructions for updating:\n[1,7]&lt;stderr&gt;:Use tf.cast instead.\n[1,7]&lt;stderr&gt;:From /databricks/python/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n[1,7]&lt;stderr&gt;:Instructions for updating:\n[1,7]&lt;stderr&gt;:Use tf.cast instead.\n[1,1]&lt;stdout&gt;:model compile is done\n[1,1]&lt;stdout&gt;:hvd call back done\n[1,4]&lt;stdout&gt;:model compile is done\n[1,4]&lt;stdout&gt;:hvd call back done\n[1,2]&lt;stdout&gt;:model compile is done\n[1,2]&lt;stdout&gt;:hvd call back done\n[1,3]&lt;stderr&gt;:WARNING:tensorflow:From /databricks/python/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n[1,3]&lt;stderr&gt;:Instructions for updating:\n[1,3]&lt;stderr&gt;:Use tf.cast instead.\n[1,3]&lt;stderr&gt;:From /databricks/python/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n[1,3]&lt;stderr&gt;:Instructions for updating:\n[1,3]&lt;stderr&gt;:Use tf.cast instead.\n[1,1]&lt;stderr&gt;:WARNING:tensorflow:From /databricks/python/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n[1,1]&lt;stderr&gt;:Instructions for updating:\n[1,1]&lt;stderr&gt;:Use tf.cast instead.\n[1,1]&lt;stderr&gt;:From /databricks/python/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n[1,1]&lt;stderr&gt;:Instructions for updating:\n[1,1]&lt;stderr&gt;:Use tf.cast instead.\n[1,4]&lt;stderr&gt;:WARNING:tensorflow:From /databricks/python/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n[1,4]&lt;stderr&gt;:Instructions for updating:\n[1,4]&lt;stderr&gt;:Use tf.cast instead.\n[1,4]&lt;stderr&gt;:From /databricks/python/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n[1,4]&lt;stderr&gt;:Instructions for updating:\n[1,4]&lt;stderr&gt;:Use tf.cast instead.\n[1,2]&lt;stderr&gt;:WARNING:tensorflow:From /databricks/python/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n[1,2]&lt;stderr&gt;:Instructions for updating:\n[1,2]&lt;stderr&gt;:Use tf.cast instead.\n[1,2]&lt;stderr&gt;:From /databricks/python/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n[1,2]&lt;stderr&gt;:Instructions for updating:\n[1,2]&lt;stderr&gt;:Use tf.cast instead.\n[1,0]&lt;stdout&gt;:Dataset has 2708 nodes, 5429 edges, 1433 features.\n[1,5]&lt;stdout&gt;:model compile is done\n[1,5]&lt;stdout&gt;:hvd call back done\n[1,0]&lt;stderr&gt;:OMP: Info #209: KMP_AFFINITY: decoding x2APIC ids.\n[1,0]&lt;stderr&gt;:OMP: Info #207: KMP_AFFINITY: Affinity capable, using global cpuid leaf 11 info\n[1,0]&lt;stderr&gt;:OMP: Info #154: KMP_AFFINITY: Initial OS proc set respected: {0,1,2,3,4,5,6,7}\n[1,0]&lt;stderr&gt;:OMP: Info #156: KMP_AFFINITY: 8 available OS procs\n[1,0]&lt;stderr&gt;:OMP: Info #157: KMP_AFFINITY: Uniform topology\n[1,0]&lt;stderr&gt;:OMP: Info #179: KMP_AFFINITY: 1 packages x 4 cores/pkg x 2 threads/core (4 total cores)\n[1,0]&lt;stderr&gt;:OMP: Info #211: KMP_AFFINITY: OS proc to physical thread map:\n[1,0]&lt;stderr&gt;:OMP: Info #171: KMP_AFFINITY: OS proc 0 maps to package 0 core 0 thread 0 \n[1,0]&lt;stderr&gt;:OMP: Info #171: KMP_AFFINITY: OS proc 4 maps to package 0 core 0 thread 1 \n[1,0]&lt;stderr&gt;:OMP: Info #171: KMP_AFFINITY: OS proc 1 maps to package 0 core 1 thread 0 \n[1,0]&lt;stderr&gt;:OMP: Info #171: KMP_AFFINITY: OS proc 5 maps to package 0 core 1 thread 1 \n[1,0]&lt;stderr&gt;:OMP: Info #171: KMP_AFFINITY: OS proc 2 maps to package 0 core 2 thread 0 \n\n*** WARNING: skipped 101016 bytes of output ***\n\n[1,4]&lt;stdout&gt;:Epoch 168/200\n[1,0]&lt;stdout&gt;: - 0s - loss: 1.0160\n[1,0]&lt;stdout&gt;:Epoch 147/200\n[1,2]&lt;stdout&gt;: - 0s - loss: 0.9866\n[1,2]&lt;stdout&gt;:Epoch 145/200\n[1,1]&lt;stdout&gt;: - 0s - loss: 1.0117\n[1,1]&lt;stdout&gt;:Epoch 158/200\n[1,6]&lt;stdout&gt;: - 0s - loss: 0.9930\n[1,6]&lt;stdout&gt;:Epoch 135/200\n[1,7]&lt;stdout&gt;: - 0s - loss: 1.0010\n[1,7]&lt;stdout&gt;:Epoch 161/200\n[1,5]&lt;stdout&gt;: - 0s - loss: 1.0753\n[1,5]&lt;stdout&gt;:Epoch 142/200\n[1,3]&lt;stdout&gt;: - 0s - loss: 0.9744\n[1,3]&lt;stdout&gt;:Epoch 160/200\n[1,4]&lt;stdout&gt;: - 0s - loss: 0.8752\n[1,4]&lt;stdout&gt;:Epoch 169/200\n[1,0]&lt;stdout&gt;: - 0s - loss: 0.9965\n[1,0]&lt;stdout&gt;:Epoch 148/200\n[1,1]&lt;stdout&gt;: - 0s - loss: 1.0000\n[1,1]&lt;stdout&gt;:Epoch 159/200\n[1,2]&lt;stdout&gt;: - 0s - loss: 0.9677\n[1,2]&lt;stdout&gt;:Epoch 146/200\n[1,6]&lt;stdout&gt;: - 0s - loss: 0.9921\n[1,6]&lt;stdout&gt;:Epoch 136/200\n[1,7]&lt;stdout&gt;: - 0s - loss: 0.9579\n[1,7]&lt;stdout&gt;:Epoch 162/200\n[1,5]&lt;stdout&gt;: - 0s - loss: 1.0957\n[1,5]&lt;stdout&gt;:Epoch 143/200\n[1,3]&lt;stdout&gt;: - 0s - loss: 1.0036\n[1,3]&lt;stdout&gt;:Epoch 161/200\n[1,4]&lt;stdout&gt;: - 0s - loss: 0.8974\n[1,4]&lt;stdout&gt;:Epoch 170/200\n[1,1]&lt;stdout&gt;: - 0s - loss: 1.0161\n[1,1]&lt;stdout&gt;:Epoch 160/200\n[1,2]&lt;stdout&gt;: - 0s - loss: 0.9806\n[1,2]&lt;stdout&gt;:Epoch 147/200\n[1,0]&lt;stdout&gt;: - 0s - loss: 1.0637\n[1,0]&lt;stdout&gt;:Epoch 149/200\n[1,6]&lt;stdout&gt;: - 0s - loss: 0.9679\n[1,6]&lt;stdout&gt;:Epoch 137/200\n[1,7]&lt;stdout&gt;: - 0s - loss: 0.9498\n[1,7]&lt;stdout&gt;:Epoch 163/200\n[1,4]&lt;stdout&gt;: - 0s - loss: 0.8298\n[1,4]&lt;stdout&gt;:Epoch 171/200\n[1,3]&lt;stdout&gt;: - 0s - loss: 0.9719\n[1,3]&lt;stdout&gt;:Epoch 162/200\n[1,5]&lt;stdout&gt;: - 0s - loss: 1.0654\n[1,5]&lt;stdout&gt;:Epoch 144/200\n[1,1]&lt;stdout&gt;: - 0s - loss: 1.0348\n[1,1]&lt;stdout&gt;:Epoch 161/200\n[1,0]&lt;stdout&gt;: - 0s - loss: 0.9856\n[1,0]&lt;stdout&gt;:Epoch 150/200\n[1,2]&lt;stdout&gt;: - 0s - loss: 0.9430\n[1,2]&lt;stdout&gt;:Epoch 148/200\n[1,7]&lt;stdout&gt;: - 0s - loss: 0.9610\n[1,7]&lt;stdout&gt;:Epoch 164/200\n[1,6]&lt;stdout&gt;: - 0s - loss: 1.0631\n[1,6]&lt;stdout&gt;:Epoch 138/200\n[1,4]&lt;stdout&gt;: - 0s - loss: 0.8404\n[1,4]&lt;stdout&gt;:Epoch 172/200\n[1,3]&lt;stdout&gt;: - 0s - loss: 0.9529\n[1,3]&lt;stdout&gt;:Epoch 163/200\n[1,5]&lt;stdout&gt;: - 0s - loss: 1.0212\n[1,5]&lt;stdout&gt;:Epoch 145/200\n[1,1]&lt;stdout&gt;: - 0s - loss: 1.0091\n[1,1]&lt;stdout&gt;:Epoch 162/200\n[1,0]&lt;stdout&gt;: - 0s - loss: 0.9919\n[1,0]&lt;stdout&gt;:Epoch 151/200\n[1,2]&lt;stdout&gt;: - 0s - loss: 0.9534\n[1,2]&lt;stdout&gt;:Epoch 149/200\n[1,7]&lt;stdout&gt;: - 0s - loss: 0.9740\n[1,7]&lt;stdout&gt;:Epoch 165/200\n[1,6]&lt;stdout&gt;: - 0s - loss: 1.0011\n[1,6]&lt;stdout&gt;:Epoch 139/200\n[1,4]&lt;stdout&gt;: - 0s - loss: 0.9495\n[1,4]&lt;stdout&gt;:Epoch 173/200\n[1,3]&lt;stdout&gt;: - 0s - loss: 0.9585\n[1,3]&lt;stdout&gt;:Epoch 164/200\n[1,1]&lt;stdout&gt;: - 0s - loss: 0.9754\n[1,1]&lt;stdout&gt;:Epoch 163/200\n[1,2]&lt;stdout&gt;: - 0s - loss: 0.9247\n[1,2]&lt;stdout&gt;:Epoch 150/200\n[1,5]&lt;stdout&gt;: - 0s - loss: 1.0583\n[1,5]&lt;stdout&gt;:Epoch 146/200\n[1,0]&lt;stdout&gt;: - 0s - loss: 0.9954\n[1,0]&lt;stdout&gt;:Epoch 152/200\n[1,7]&lt;stdout&gt;: - 0s - loss: 0.9304\n[1,7]&lt;stdout&gt;:Epoch 166/200\n[1,4]&lt;stdout&gt;: - 0s - loss: 0.9170\n[1,4]&lt;stdout&gt;:Epoch 174/200\n[1,3]&lt;stdout&gt;: - 0s - loss: 0.9480\n[1,3]&lt;stdout&gt;:Epoch 165/200\n[1,6]&lt;stdout&gt;: - 0s - loss: 0.9395\n[1,6]&lt;stdout&gt;:Epoch 140/200\n[1,1]&lt;stdout&gt;: - 0s - loss: 0.9572\n[1,1]&lt;stdout&gt;:Epoch 164/200\n[1,2]&lt;stdout&gt;: - 0s - loss: 0.9556\n[1,2]&lt;stdout&gt;:Epoch 151/200\n[1,5]&lt;stdout&gt;: - 0s - loss: 1.0448\n[1,5]&lt;stdout&gt;:Epoch 147/200\n[1,0]&lt;stdout&gt;: - 0s - loss: 1.0304\n[1,0]&lt;stdout&gt;:Epoch 153/200\n[1,7]&lt;stdout&gt;: - 0s - loss: 0.9311\n[1,7]&lt;stdout&gt;:Epoch 167/200\n[1,4]&lt;stdout&gt;: - 0s - loss: 0.8100\n[1,4]&lt;stdout&gt;:Epoch 175/200\n[1,3]&lt;stdout&gt;: - 0s - loss: 0.9838\n[1,3]&lt;stdout&gt;:Epoch 166/200\n[1,6]&lt;stdout&gt;: - 0s - loss: 1.0123\n[1,6]&lt;stdout&gt;:Epoch 141/200\n[1,1]&lt;stdout&gt;: - 0s - loss: 0.9578\n[1,1]&lt;stdout&gt;:Epoch 165/200\n[1,2]&lt;stdout&gt;: - 0s - loss: 0.9080\n[1,2]&lt;stdout&gt;:Epoch 152/200\n[1,5]&lt;stdout&gt;: - 0s - loss: 1.0280\n[1,5]&lt;stdout&gt;:Epoch 148/200\n[1,7]&lt;stdout&gt;: - 0s - loss: 0.9529\n[1,7]&lt;stdout&gt;:Epoch 168/200\n[1,0]&lt;stdout&gt;: - 0s - loss: 1.0110\n[1,0]&lt;stdout&gt;:Epoch 154/200\n[1,4]&lt;stdout&gt;: - 0s - loss: 0.8504\n[1,4]&lt;stdout&gt;:Epoch 176/200\n[1,3]&lt;stdout&gt;: - 0s - loss: 0.9553\n[1,3]&lt;stdout&gt;:Epoch 167/200\n[1,6]&lt;stdout&gt;: - 0s - loss: 0.9286\n[1,6]&lt;stdout&gt;:Epoch 142/200\n[1,1]&lt;stdout&gt;: - 0s - loss: 0.9602\n[1,1]&lt;stdout&gt;:Epoch 166/200\n[1,2]&lt;stdout&gt;: - 0s - loss: 0.9436\n[1,2]&lt;stdout&gt;:Epoch 153/200\n[1,7]&lt;stdout&gt;: - 0s - loss: 0.9016\n[1,7]&lt;stdout&gt;:Epoch 169/200\n[1,5]&lt;stdout&gt;: - 0s - loss: 1.0703\n[1,5]&lt;stdout&gt;:Epoch 149/200\n[1,4]&lt;stdout&gt;: - 0s - loss: 0.8743\n[1,4]&lt;stdout&gt;:Epoch 177/200\n[1,0]&lt;stdout&gt;: - 0s - loss: 0.9850\n[1,0]&lt;stdout&gt;:Epoch 155/200\n[1,3]&lt;stdout&gt;: - 0s - loss: 0.9595\n[1,3]&lt;stdout&gt;:Epoch 168/200\n[1,1]&lt;stdout&gt;: - 0s - loss: 0.9955\n[1,1]&lt;stdout&gt;:Epoch 167/200\n[1,2]&lt;stdout&gt;: - 0s - loss: 0.9653\n[1,2]&lt;stdout&gt;:Epoch 154/200\n[1,6]&lt;stdout&gt;: - 0s - loss: 0.9960\n[1,6]&lt;stdout&gt;:Epoch 143/200\n[1,4]&lt;stdout&gt;: - 0s - loss: 0.8614\n[1,4]&lt;stdout&gt;:Epoch 178/200\n[1,7]&lt;stdout&gt;: - 0s - loss: 0.9548\n[1,7]&lt;stdout&gt;:Epoch 170/200\n[1,5]&lt;stdout&gt;: - 0s - loss: 1.0431\n[1,5]&lt;stdout&gt;:Epoch 150/200\n[1,0]&lt;stdout&gt;: - 0s - loss: 1.0048\n[1,0]&lt;stdout&gt;:Epoch 156/200\n[1,3]&lt;stdout&gt;: - 0s - loss: 0.9502\n[1,3]&lt;stdout&gt;:Epoch 169/200\n[1,1]&lt;stdout&gt;: - 0s - loss: 0.9553\n[1,1]&lt;stdout&gt;:Epoch 168/200\n[1,2]&lt;stdout&gt;: - 0s - loss: 0.9279\n[1,2]&lt;stdout&gt;:Epoch 155/200\n[1,6]&lt;stdout&gt;: - 0s - loss: 1.0017\n[1,6]&lt;stdout&gt;:Epoch 144/200\n[1,4]&lt;stdout&gt;: - 0s - loss: 0.8836\n[1,4]&lt;stdout&gt;:Epoch 179/200\n[1,7]&lt;stdout&gt;: - 0s - loss: 0.8671\n[1,7]&lt;stdout&gt;:Epoch 171/200\n[1,3]&lt;stdout&gt;: - 0s - loss: 0.9043\n[1,3]&lt;stdout&gt;:Epoch 170/200\n[1,0]&lt;stdout&gt;: - 0s - loss: 1.0671\n[1,0]&lt;stdout&gt;:Epoch 157/200\n[1,5]&lt;stdout&gt;: - 0s - loss: 1.0348\n[1,5]&lt;stdout&gt;:Epoch 151/200\n[1,1]&lt;stdout&gt;: - 0s - loss: 0.9734\n[1,1]&lt;stdout&gt;:Epoch 169/200\n[1,2]&lt;stdout&gt;: - 0s - loss: 0.9462\n[1,2]&lt;stdout&gt;:Epoch 156/200\n[1,4]&lt;stdout&gt;: - 0s - loss: 0.8423\n[1,4]&lt;stdout&gt;:Epoch 180/200\n[1,6]&lt;stdout&gt;: - 0s - loss: 0.9551\n[1,6]&lt;stdout&gt;:Epoch 145/200\n[1,7]&lt;stdout&gt;: - 0s - loss: 0.9390\n[1,7]&lt;stdout&gt;:Epoch 172/200\n[1,3]&lt;stdout&gt;: - 0s - loss: 0.9496\n[1,3]&lt;stdout&gt;:Epoch 171/200\n[1,5]&lt;stdout&gt;: - 0s - loss: 1.0258\n[1,5]&lt;stdout&gt;:Epoch 152/200\n[1,0]&lt;stdout&gt;: - 0s - loss: 0.9349\n[1,0]&lt;stdout&gt;:Epoch 158/200\n[1,1]&lt;stdout&gt;: - 0s - loss: 0.9974\n[1,1]&lt;stdout&gt;:Epoch 170/200\n[1,2]&lt;stdout&gt;: - 0s - loss: 0.9166\n[1,2]&lt;stdout&gt;:Epoch 157/200\n[1,4]&lt;stdout&gt;: - 0s - loss: 0.8577\n[1,4]&lt;stdout&gt;:Epoch 181/200\n[1,7]&lt;stdout&gt;: - 0s - loss: 0.9309\n[1,7]&lt;stdout&gt;:Epoch 173/200\n[1,3]&lt;stdout&gt;: - 0s - loss: 0.9391\n[1,3]&lt;stdout&gt;:Epoch 172/200\n[1,6]&lt;stdout&gt;: - 0s - loss: 0.9606\n[1,6]&lt;stdout&gt;:Epoch 146/200\n[1,0]&lt;stdout&gt;: - 0s - loss: 1.0383\n[1,0]&lt;stdout&gt;:Epoch 159/200\n[1,5]&lt;stdout&gt;: - 0s - loss: 1.0453\n[1,5]&lt;stdout&gt;:Epoch 153/200\n[1,1]&lt;stdout&gt;: - 0s - loss: 0.9750\n[1,1]&lt;stdout&gt;:Epoch 171/200\n[1,2]&lt;stdout&gt;: - 0s - loss: 0.9073\n[1,2]&lt;stdout&gt;:Epoch 158/200\n[1,4]&lt;stdout&gt;: - 0s - loss: 0.8362\n[1,4]&lt;stdout&gt;:Epoch 182/200\n[1,7]&lt;stdout&gt;: - 0s - loss: 0.9248\n[1,7]&lt;stdout&gt;:Epoch 174/200\n[1,3]&lt;stdout&gt;: - 0s - loss: 0.9105\n[1,3]&lt;stdout&gt;:Epoch 173/200\n[1,6]&lt;stdout&gt;: - 0s - loss: 0.9845\n[1,6]&lt;stdout&gt;:Epoch 147/200\n[1,0]&lt;stdout&gt;: - 0s - loss: 1.0168\n[1,0]&lt;stdout&gt;:Epoch 160/200\n[1,1]&lt;stdout&gt;: - 0s - loss: 0.9739\n[1,1]&lt;stdout&gt;:Epoch 172/200\n[1,5]&lt;stdout&gt;: - 0s - loss: 1.0282\n[1,5]&lt;stdout&gt;:Epoch 154/200\n[1,2]&lt;stdout&gt;: - 0s - loss: 0.9339\n[1,2]&lt;stdout&gt;:Epoch 159/200\n[1,4]&lt;stdout&gt;: - 0s - loss: 0.8420\n[1,4]&lt;stdout&gt;:Epoch 183/200\n[1,7]&lt;stdout&gt;: - 0s - loss: 0.9814\n[1,7]&lt;stdout&gt;:Epoch 175/200\n[1,3]&lt;stdout&gt;: - 0s - loss: 0.9362\n[1,3]&lt;stdout&gt;:Epoch 174/200\n[1,6]&lt;stdout&gt;: - 0s - loss: 0.9586\n[1,6]&lt;stdout&gt;:Epoch 148/200\n[1,1]&lt;stdout&gt;: - 0s - loss: 0.8999\n[1,1]&lt;stdout&gt;:Epoch 173/200\n[1,0]&lt;stdout&gt;: - 0s - loss: 0.9781\n[1,0]&lt;stdout&gt;:Epoch 161/200\n[1,5]&lt;stdout&gt;: - 0s - loss: 1.0316\n[1,5]&lt;stdout&gt;:Epoch 155/200\n[1,2]&lt;stdout&gt;: - 0s - loss: 0.9286\n[1,2]&lt;stdout&gt;:Epoch 160/200\n[1,4]&lt;stdout&gt;: - 0s - loss: 0.8277\n[1,4]&lt;stdout&gt;:Epoch 184/200\n[1,7]&lt;stdout&gt;: - 0s - loss: 0.9201\n[1,7]&lt;stdout&gt;:Epoch 176/200\n[1,3]&lt;stdout&gt;: - 0s - loss: 0.9051\n[1,3]&lt;stdout&gt;:Epoch 175/200\n[1,1]&lt;stdout&gt;: - 0s - loss: 1.0183\n[1,1]&lt;stdout&gt;:Epoch 174/200\n[1,0]&lt;stdout&gt;: - 0s - loss: 0.9532\n[1,0]&lt;stdout&gt;:Epoch 162/200\n[1,4]&lt;stdout&gt;: - 0s - loss: 0.8598\n[1,4]&lt;stdout&gt;:Epoch 185/200\n[1,6]&lt;stdout&gt;: - 0s - loss: 0.9559\n[1,6]&lt;stdout&gt;:Epoch 149/200\n[1,2]&lt;stdout&gt;: - 0s - loss: 0.9122\n[1,2]&lt;stdout&gt;:Epoch 161/200\n[1,5]&lt;stdout&gt;: - 0s - loss: 0.9794\n[1,5]&lt;stdout&gt;:Epoch 156/200\n[1,7]&lt;stdout&gt;: - 0s - loss: 0.8940\n[1,7]&lt;stdout&gt;:Epoch 177/200\n[1,3]&lt;stdout&gt;: - 0s - loss: 0.9581\n[1,3]&lt;stdout&gt;:Epoch 176/200\n[1,1]&lt;stdout&gt;: - 0s - loss: 0.9418\n[1,1]&lt;stdout&gt;:Epoch 175/200\n[1,4]&lt;stdout&gt;: - 0s - loss: 0.8600\n[1,4]&lt;stdout&gt;:Epoch 186/200\n[1,0]&lt;stdout&gt;: - 0s - loss: 1.0252\n[1,0]&lt;stdout&gt;:Epoch 163/200\n[1,2]&lt;stdout&gt;: - 0s - loss: 0.9414\n[1,2]&lt;stdout&gt;:Epoch 162/200\n[1,6]&lt;stdout&gt;: - 0s - loss: 0.9764\n[1,6]&lt;stdout&gt;:Epoch 150/200\n[1,5]&lt;stdout&gt;: - 0s - loss: 1.0402\n[1,5]&lt;stdout&gt;:Epoch 157/200\n[1,7]&lt;stdout&gt;: - 0s - loss: 0.9042\n[1,7]&lt;stdout&gt;:Epoch 178/200\n[1,3]&lt;stdout&gt;: - 0s - loss: 0.8951\n[1,3]&lt;stdout&gt;:Epoch 177/200\n[1,1]&lt;stdout&gt;: - 0s - loss: 0.9216\n[1,1]&lt;stdout&gt;:Epoch 176/200\n[1,4]&lt;stdout&gt;: - 0s - loss: 0.8329\n[1,4]&lt;stdout&gt;:Epoch 187/200\n[1,2]&lt;stdout&gt;: - 0s - loss: 0.8749\n[1,2]&lt;stdout&gt;:Epoch 163/200\n[1,0]&lt;stdout&gt;: - 0s - loss: 0.9562\n[1,0]&lt;stdout&gt;:Epoch 164/200\n[1,5]&lt;stdout&gt;: - 0s - loss: 1.0426\n[1,5]&lt;stdout&gt;:Epoch 158/200\n[1,6]&lt;stdout&gt;: - 0s - loss: 0.9694\n[1,6]&lt;stdout&gt;:Epoch 151/200\n[1,3]&lt;stdout&gt;: - 0s - loss: 0.8927\n[1,3]&lt;stdout&gt;:Epoch 178/200\n[1,7]&lt;stdout&gt;: - 0s - loss: 0.9455\n[1,7]&lt;stdout&gt;:Epoch 179/200\n[1,1]&lt;stdout&gt;: - 0s - loss: 0.9145\n[1,1]&lt;stdout&gt;:Epoch 177/200\n[1,4]&lt;stdout&gt;: - 0s - loss: 0.8150\n[1,4]&lt;stdout&gt;:Epoch 188/200\n[1,2]&lt;stdout&gt;: - 0s - loss: 1.0062\n[1,2]&lt;stdout&gt;:Epoch 164/200\n[1,0]&lt;stdout&gt;: - 0s - loss: 0.9879\n[1,0]&lt;stdout&gt;:Epoch 165/200\n[1,3]&lt;stdout&gt;: - 0s - loss: 1.0039\n[1,3]&lt;stdout&gt;:Epoch 179/200\n[1,5]&lt;stdout&gt;: - 0s - loss: 1.0340\n[1,5]&lt;stdout&gt;:Epoch 159/200\n[1,7]&lt;stdout&gt;: - 0s - loss: 0.9062\n[1,7]&lt;stdout&gt;:Epoch 180/200\n[1,6]&lt;stdout&gt;: - 0s - loss: 0.9364\n[1,6]&lt;stdout&gt;:Epoch 152/200\n[1,4]&lt;stdout&gt;: - 0s - loss: 0.8302\n[1,4]&lt;stdout&gt;:Epoch 189/200\n[1,1]&lt;stdout&gt;: - 0s - loss: 0.9595\n[1,1]&lt;stdout&gt;:Epoch 178/200\n[1,2]&lt;stdout&gt;: - 0s - loss: 0.9077\n[1,2]&lt;stdout&gt;:Epoch 165/200\n[1,0]&lt;stdout&gt;: - 0s - loss: 1.0139\n[1,0]&lt;stdout&gt;:Epoch 166/200\n[1,3]&lt;stdout&gt;: - 0s - loss: 0.9404\n[1,3]&lt;stdout&gt;:Epoch 180/200\n[1,5]&lt;stdout&gt;: - 0s - loss: 1.0020\n[1,5]&lt;stdout&gt;:Epoch 160/200\n[1,7]&lt;stdout&gt;: - 0s - loss: 0.8734\n[1,7]&lt;stdout&gt;:Epoch 181/200\n[1,6]&lt;stdout&gt;: - 0s - loss: 0.9060\n[1,6]&lt;stdout&gt;:Epoch 153/200\n[1,4]&lt;stdout&gt;: - 0s - loss: 0.8079\n[1,4]&lt;stdout&gt;:Epoch 190/200\n[1,1]&lt;stdout&gt;: - 0s - loss: 0.9553\n[1,1]&lt;stdout&gt;:Epoch 179/200\n[1,2]&lt;stdout&gt;: - 0s - loss: 0.9421\n[1,2]&lt;stdout&gt;:Epoch 166/200\n[1,0]&lt;stdout&gt;: - 0s - loss: 0.9606\n[1,0]&lt;stdout&gt;:Epoch 167/200\n[1,3]&lt;stdout&gt;: - 0s - loss: 0.8834\n[1,3]&lt;stdout&gt;:Epoch 181/200\n[1,5]&lt;stdout&gt;: - 0s - loss: 0.9693\n[1,5]&lt;stdout&gt;:Epoch 161/200\n[1,7]&lt;stdout&gt;: - 0s - loss: 0.9074\n[1,7]&lt;stdout&gt;:Epoch 182/200\n[1,6]&lt;stdout&gt;: - 0s - loss: 0.9800\n[1,6]&lt;stdout&gt;:Epoch 154/200\n[1,4]&lt;stdout&gt;: - 0s - loss: 0.8856\n[1,4]&lt;stdout&gt;:Epoch 191/200\n[1,1]&lt;stdout&gt;: - 0s - loss: 0.9760\n[1,1]&lt;stdout&gt;:Epoch 180/200\n[1,2]&lt;stdout&gt;: - 0s - loss: 0.9443\n[1,2]&lt;stdout&gt;:Epoch 167/200\n[1,0]&lt;stdout&gt;: - 0s - loss: 1.0134\n[1,0]&lt;stdout&gt;:Epoch 168/200\n[1,3]&lt;stdout&gt;: - 0s - loss: 0.8898\n[1,3]&lt;stdout&gt;:Epoch 182/200\n[1,7]&lt;stdout&gt;: - 0s - loss: 0.9171\n[1,7]&lt;stdout&gt;:Epoch 183/200\n[1,5]&lt;stdout&gt;: - 0s - loss: 0.9704\n[1,5]&lt;stdout&gt;:Epoch 162/200\n[1,4]&lt;stdout&gt;: - 0s - loss: 0.7968\n[1,4]&lt;stdout&gt;:Epoch 192/200\n[1,1]&lt;stdout&gt;: - 0s - loss: 0.9124\n[1,1]&lt;stdout&gt;:Epoch 181/200\n[1,6]&lt;stdout&gt;: - 0s - loss: 0.9220\n[1,6]&lt;stdout&gt;:Epoch 155/200\n[1,2]&lt;stdout&gt;: - 0s - loss: 0.9273\n[1,2]&lt;stdout&gt;:Epoch 168/200\n[1,0]&lt;stdout&gt;: - 0s - loss: 0.9396\n[1,0]&lt;stdout&gt;:Epoch 169/200\n[1,3]&lt;stdout&gt;: - 0s - loss: 0.9044\n[1,3]&lt;stdout&gt;:Epoch 183/200\n[1,7]&lt;stdout&gt;: - 0s - loss: 0.8938\n[1,7]&lt;stdout&gt;:Epoch 184/200\n[1,5]&lt;stdout&gt;: - 0s - loss: 0.9472\n[1,5]&lt;stdout&gt;:Epoch 163/200\n[1,4]&lt;stdout&gt;: - 0s - loss: 0.8787\n[1,4]&lt;stdout&gt;:Epoch 193/200\n[1,1]&lt;stdout&gt;: - 0s - loss: 0.9177\n[1,1]&lt;stdout&gt;:Epoch 182/200\n[1,6]&lt;stdout&gt;: - 0s - loss: 0.8887\n[1,6]&lt;stdout&gt;:Epoch 156/200\n[1,2]&lt;stdout&gt;: - 0s - loss: 0.9652\n[1,2]&lt;stdout&gt;:Epoch 169/200\n[1,3]&lt;stdout&gt;: - 0s - loss: 0.8841\n[1,3]&lt;stdout&gt;:Epoch 184/200\n[1,0]&lt;stdout&gt;: - 0s - loss: 0.9546\n[1,0]&lt;stdout&gt;:Epoch 170/200\n[1,7]&lt;stdout&gt;: - 0s - loss: 0.8803\n[1,7]&lt;stdout&gt;:Epoch 185/200\n[1,4]&lt;stdout&gt;: - 0s - loss: 0.8106\n[1,4]&lt;stdout&gt;:Epoch 194/200\n[1,5]&lt;stdout&gt;: - 0s - loss: 0.9942\n[1,5]&lt;stdout&gt;:Epoch 164/200\n[1,1]&lt;stdout&gt;: - 0s - loss: 0.9347\n[1,1]&lt;stdout&gt;:Epoch 183/200\n[1,2]&lt;stdout&gt;: - 0s - loss: 0.8853\n[1,2]&lt;stdout&gt;:Epoch 170/200\n[1,6]&lt;stdout&gt;: - 0s - loss: 0.9837\n[1,6]&lt;stdout&gt;:Epoch 157/200\n[1,3]&lt;stdout&gt;: - 0s - loss: 0.9441\n[1,3]&lt;stdout&gt;:Epoch 185/200\n[1,7]&lt;stdout&gt;: - 0s - loss: 0.8842\n[1,7]&lt;stdout&gt;:Epoch 186/200\n[1,0]&lt;stdout&gt;: - 0s - loss: 0.9214\n[1,0]&lt;stdout&gt;:Epoch 171/200\n[1,4]&lt;stdout&gt;: - 0s - loss: 0.8563\n[1,4]&lt;stdout&gt;:Epoch 195/200\n[1,5]&lt;stdout&gt;: - 0s - loss: 1.0045\n[1,5]&lt;stdout&gt;:Epoch 165/200\n[1,1]&lt;stdout&gt;: - 0s - loss: 0.9534\n[1,1]&lt;stdout&gt;:Epoch 184/200\n[1,2]&lt;stdout&gt;: - 0s - loss: 0.9652\n[1,2]&lt;stdout&gt;:Epoch 171/200\n[1,6]&lt;stdout&gt;: - 0s - loss: 0.9535\n[1,6]&lt;stdout&gt;:Epoch 158/200\n[1,3]&lt;stdout&gt;: - 0s - loss: 0.9373\n[1,3]&lt;stdout&gt;:Epoch 186/200\n[1,7]&lt;stdout&gt;: - 0s - loss: 0.8668\n[1,7]&lt;stdout&gt;:Epoch 187/200\n[1,4]&lt;stdout&gt;: - 0s - loss: 0.8063\n[1,4]&lt;stdout&gt;:Epoch 196/200\n[1,0]&lt;stdout&gt;: - 0s - loss: 0.9933\n[1,0]&lt;stdout&gt;:Epoch 172/200\n[1,1]&lt;stdout&gt;: - 0s - loss: 0.9312\n[1,1]&lt;stdout&gt;:Epoch 185/200\n[1,5]&lt;stdout&gt;: - 0s - loss: 0.9795\n[1,5]&lt;stdout&gt;:Epoch 166/200\n[1,2]&lt;stdout&gt;: - 0s - loss: 0.9027\n[1,2]&lt;stdout&gt;:Epoch 172/200\n[1,3]&lt;stdout&gt;: - 0s - loss: 0.8926\n[1,3]&lt;stdout&gt;:Epoch 187/200\n[1,7]&lt;stdout&gt;: - 0s - loss: 0.9199\n[1,7]&lt;stdout&gt;:Epoch 188/200\n[1,6]&lt;stdout&gt;: - 0s - loss: 0.9011\n[1,6]&lt;stdout&gt;:Epoch 159/200\n[1,4]&lt;stdout&gt;: - 0s - loss: 0.7991\n[1,4]&lt;stdout&gt;:Epoch 197/200\n[1,1]&lt;stdout&gt;: - 0s - loss: 0.9244\n[1,1]&lt;stdout&gt;:Epoch 186/200\n[1,0]&lt;stdout&gt;: - 0s - loss: 0.9611\n[1,0]&lt;stdout&gt;:Epoch 173/200\n[1,5]&lt;stdout&gt;: - 0s - loss: 1.0312\n[1,5]&lt;stdout&gt;:Epoch 167/200\n[1,2]&lt;stdout&gt;: - 0s - loss: 0.8503\n[1,2]&lt;stdout&gt;:Epoch 173/200\n[1,3]&lt;stdout&gt;: - 0s - loss: 0.8953\n[1,3]&lt;stdout&gt;:Epoch 188/200\n[1,7]&lt;stdout&gt;: - 0s - loss: 0.9370\n[1,7]&lt;stdout&gt;:Epoch 189/200\n[1,4]&lt;stdout&gt;: - 0s - loss: 0.8741\n[1,4]&lt;stdout&gt;:Epoch 198/200\n[1,6]&lt;stdout&gt;: - 0s - loss: 0.9215\n[1,6]&lt;stdout&gt;:Epoch 160/200\n[1,1]&lt;stdout&gt;: - 0s - loss: 0.8981\n[1,1]&lt;stdout&gt;:Epoch 187/200\n[1,5]&lt;stdout&gt;: - 0s - loss: 1.0196\n[1,5]&lt;stdout&gt;:Epoch 168/200\n[1,0]&lt;stdout&gt;: - 0s - loss: 0.9746\n[1,0]&lt;stdout&gt;:Epoch 174/200\n[1,2]&lt;stdout&gt;: - 0s - loss: 0.9034\n[1,2]&lt;stdout&gt;:Epoch 174/200\n[1,3]&lt;stdout&gt;: - 0s - loss: 0.8613\n[1,3]&lt;stdout&gt;:Epoch 189/200\n[1,4]&lt;stdout&gt;: - 0s - loss: 0.7974\n[1,4]&lt;stdout&gt;:Epoch 199/200\n[1,7]&lt;stdout&gt;: - 0s - loss: 0.8942\n[1,7]&lt;stdout&gt;:Epoch 190/200\n[1,1]&lt;stdout&gt;: - 0s - loss: 0.9039\n[1,1]&lt;stdout&gt;:Epoch 188/200\n[1,6]&lt;stdout&gt;: - 0s - loss: 0.9618\n[1,6]&lt;stdout&gt;:Epoch 161/200\n[1,5]&lt;stdout&gt;: - 0s - loss: 0.9768\n[1,5]&lt;stdout&gt;:Epoch 169/200\n[1,2]&lt;stdout&gt;: - 0s - loss: 0.8937\n[1,2]&lt;stdout&gt;:Epoch 175/200\n[1,0]&lt;stdout&gt;: - 0s - loss: 0.9752\n[1,0]&lt;stdout&gt;:Epoch 175/200\n[1,3]&lt;stdout&gt;: - 0s - loss: 0.9251\n[1,3]&lt;stdout&gt;:Epoch 190/200\n[1,4]&lt;stdout&gt;: - 0s - loss: 0.7989\n[1,4]&lt;stdout&gt;:Epoch 200/200\n[1,7]&lt;stdout&gt;: - 0s - loss: 0.8875\n[1,7]&lt;stdout&gt;:Epoch 191/200\n[1,1]&lt;stdout&gt;: - 0s - loss: 0.9264\n[1,1]&lt;stdout&gt;:Epoch 189/200\n[1,6]&lt;stdout&gt;: - 0s - loss: 0.9189\n[1,6]&lt;stdout&gt;:Epoch 162/200\n[1,5]&lt;stdout&gt;: - 0s - loss: 0.9379\n[1,5]&lt;stdout&gt;:Epoch 170/200\n[1,2]&lt;stdout&gt;: - 0s - loss: 0.8991\n[1,2]&lt;stdout&gt;:Epoch 176/200\n[1,0]&lt;stdout&gt;: - 0s - loss: 0.9095\n[1,0]&lt;stdout&gt;:Epoch 176/200\n[1,3]&lt;stdout&gt;: - 0s - loss: 0.9138\n[1,3]&lt;stdout&gt;:Epoch 191/200\n[1,4]&lt;stdout&gt;: - 0s - loss: 0.8306\n[1,7]&lt;stdout&gt;: - 0s - loss: 0.9020\n[1,7]&lt;stdout&gt;:Epoch 192/200\n[1,1]&lt;stdout&gt;: - 0s - loss: 0.8943\n[1,1]&lt;stdout&gt;:Epoch 190/200\n[1,2]&lt;stdout&gt;: - 0s - loss: 0.9182\n[1,2]&lt;stdout&gt;:Epoch 177/200\n[1,5]&lt;stdout&gt;: - 0s - loss: 0.9763\n[1,5]&lt;stdout&gt;:Epoch 171/200\n[1,6]&lt;stdout&gt;: - 0s - loss: 0.9119\n[1,6]&lt;stdout&gt;:Epoch 163/200\n[1,0]&lt;stdout&gt;: - 0s - loss: 0.9413\n[1,0]&lt;stdout&gt;:Epoch 177/200\n[1,3]&lt;stdout&gt;: - 0s - loss: 0.9048\n[1,3]&lt;stdout&gt;:Epoch 192/200\n[1,7]&lt;stdout&gt;: - 0s - loss: 0.8618\n[1,7]&lt;stdout&gt;:Epoch 193/200\n[1,1]&lt;stdout&gt;: - 0s - loss: 0.9332\n[1,1]&lt;stdout&gt;:Epoch 191/200\n[1,2]&lt;stdout&gt;: - 0s - loss: 0.9218\n[1,2]&lt;stdout&gt;:Epoch 178/200\n[1,5]&lt;stdout&gt;: - 0s - loss: 0.9702\n[1,5]&lt;stdout&gt;:Epoch 172/200\n[1,3]&lt;stdout&gt;: - 0s - loss: 0.8647\n[1,3]&lt;stdout&gt;:Epoch 193/200\n[1,0]&lt;stdout&gt;: - 0s - loss: 0.9180\n[1,0]&lt;stdout&gt;:Epoch 178/200\n[1,6]&lt;stdout&gt;: - 0s - loss: 0.9372\n[1,6]&lt;stdout&gt;:Epoch 164/200\n[1,7]&lt;stdout&gt;: - 0s - loss: 0.8868\n[1,7]&lt;stdout&gt;:Epoch 194/200\n[1,1]&lt;stdout&gt;: - 0s - loss: 0.9044\n[1,1]&lt;stdout&gt;:Epoch 192/200\n[1,2]&lt;stdout&gt;: - 0s - loss: 0.9225\n[1,2]&lt;stdout&gt;:Epoch 179/200\n[1,5]&lt;stdout&gt;: - 0s - loss: 1.0493\n[1,5]&lt;stdout&gt;:Epoch 173/200\n[1,3]&lt;stdout&gt;: - 0s - loss: 0.9091\n[1,3]&lt;stdout&gt;:Epoch 194/200\n[1,0]&lt;stdout&gt;: - 0s - loss: 0.9352\n[1,0]&lt;stdout&gt;:Epoch 179/200\n[1,6]&lt;stdout&gt;: - 0s - loss: 0.9230\n[1,6]&lt;stdout&gt;:Epoch 165/200\n[1,7]&lt;stdout&gt;: - 0s - loss: 0.8899\n[1,7]&lt;stdout&gt;:Epoch 195/200\n[1,1]&lt;stdout&gt;: - 0s - loss: 0.9003\n[1,1]&lt;stdout&gt;:Epoch 193/200\n[1,2]&lt;stdout&gt;: - 0s - loss: 0.9015\n[1,2]&lt;stdout&gt;:Epoch 180/200\n[1,3]&lt;stdout&gt;: - 0s - loss: 0.8592\n[1,3]&lt;stdout&gt;:Epoch 195/200\n[1,5]&lt;stdout&gt;: - 0s - loss: 0.9461\n[1,5]&lt;stdout&gt;:Epoch 174/200\n[1,0]&lt;stdout&gt;: - 0s - loss: 0.9179\n[1,0]&lt;stdout&gt;:Epoch 180/200\n[1,7]&lt;stdout&gt;: - 0s - loss: 0.9171\n[1,7]&lt;stdout&gt;:Epoch 196/200\n[1,6]&lt;stdout&gt;: - 0s - loss: 0.9010\n[1,6]&lt;stdout&gt;:Epoch 166/200\n[1,1]&lt;stdout&gt;: - 0s - loss: 0.9169\n[1,1]&lt;stdout&gt;:Epoch 194/200\n[1,2]&lt;stdout&gt;: - 0s - loss: 0.9100\n[1,2]&lt;stdout&gt;:Epoch 181/200\n[1,3]&lt;stdout&gt;: - 0s - loss: 0.8520\n[1,3]&lt;stdout&gt;:Epoch 196/200\n[1,5]&lt;stdout&gt;: - 0s - loss: 0.9704\n[1,5]&lt;stdout&gt;:Epoch 175/200\n[1,0]&lt;stdout&gt;: - 0s - loss: 0.9066\n[1,0]&lt;stdout&gt;:Epoch 181/200\n[1,7]&lt;stdout&gt;: - 0s - loss: 0.8794\n[1,7]&lt;stdout&gt;:Epoch 197/200\n[1,6]&lt;stdout&gt;: - 0s - loss: 0.9411\n[1,6]&lt;stdout&gt;:Epoch 167/200\n[1,1]&lt;stdout&gt;: - 0s - loss: 0.8805\n[1,1]&lt;stdout&gt;:Epoch 195/200\n[1,3]&lt;stdout&gt;: - 0s - loss: 0.8586\n[1,3]&lt;stdout&gt;:Epoch 197/200\n[1,2]&lt;stdout&gt;: - 0s - loss: 0.8655\n[1,2]&lt;stdout&gt;:Epoch 182/200\n[1,5]&lt;stdout&gt;: - 0s - loss: 0.9904\n[1,5]&lt;stdout&gt;:Epoch 176/200\n[1,0]&lt;stdout&gt;: - 0s - loss: 0.9047\n[1,0]&lt;stdout&gt;:Epoch 182/200\n[1,7]&lt;stdout&gt;: - 0s - loss: 0.8758\n[1,7]&lt;stdout&gt;:Epoch 198/200\n[1,1]&lt;stdout&gt;: - 0s - loss: 0.9103\n[1,1]&lt;stdout&gt;:Epoch 196/200\n[1,6]&lt;stdout&gt;: - 0s - loss: 0.9168\n[1,6]&lt;stdout&gt;:Epoch 168/200\n[1,3]&lt;stdout&gt;: - 0s - loss: 0.8617\n[1,3]&lt;stdout&gt;:Epoch 198/200\n[1,2]&lt;stdout&gt;: - 0s - loss: 0.8821\n[1,2]&lt;stdout&gt;:Epoch 183/200\n[1,5]&lt;stdout&gt;: - 0s - loss: 0.9408\n[1,5]&lt;stdout&gt;:Epoch 177/200\n[1,7]&lt;stdout&gt;: - 0s - loss: 0.8629\n[1,7]&lt;stdout&gt;:Epoch 199/200\n[1,0]&lt;stdout&gt;: - 0s - loss: 0.9218\n[1,0]&lt;stdout&gt;:Epoch 183/200\n[1,1]&lt;stdout&gt;: - 0s - loss: 0.9386\n[1,1]&lt;stdout&gt;:Epoch 197/200\n[1,6]&lt;stdout&gt;: - 0s - loss: 0.9094\n[1,6]&lt;stdout&gt;:Epoch 169/200\n[1,3]&lt;stdout&gt;: - 0s - loss: 0.8577\n[1,3]&lt;stdout&gt;:Epoch 199/200\n[1,2]&lt;stdout&gt;: - 0s - loss: 0.8600\n[1,2]&lt;stdout&gt;:Epoch 184/200\n[1,5]&lt;stdout&gt;: - 0s - loss: 0.9888\n[1,5]&lt;stdout&gt;:Epoch 178/200\n[1,7]&lt;stdout&gt;: - 0s - loss: 0.8660\n[1,7]&lt;stdout&gt;:Epoch 200/200\n[1,0]&lt;stdout&gt;: - 0s - loss: 0.8965\n[1,0]&lt;stdout&gt;:Epoch 184/200\n[1,1]&lt;stdout&gt;: - 0s - loss: 0.9152\n[1,1]&lt;stdout&gt;:Epoch 198/200\n[1,6]&lt;stdout&gt;: - 0s - loss: 0.8578\n[1,6]&lt;stdout&gt;:Epoch 170/200\n[1,3]&lt;stdout&gt;: - 0s - loss: 0.8708\n[1,3]&lt;stdout&gt;:Epoch 200/200\n[1,2]&lt;stdout&gt;: - 0s - loss: 0.9185\n[1,2]&lt;stdout&gt;:Epoch 185/200\n[1,7]&lt;stdout&gt;: - 0s - loss: 0.8837\n[1,5]&lt;stdout&gt;: - 0s - loss: 1.0398\n[1,5]&lt;stdout&gt;:Epoch 179/200\n[1,0]&lt;stdout&gt;: - 0s - loss: 0.9572\n[1,0]&lt;stdout&gt;:Epoch 185/200\n[1,1]&lt;stdout&gt;: - 0s - loss: 0.9217\n[1,1]&lt;stdout&gt;:Epoch 199/200\n[1,6]&lt;stdout&gt;: - 0s - loss: 0.8792\n[1,6]&lt;stdout&gt;:Epoch 171/200\n[1,3]&lt;stdout&gt;: - 0s - loss: 0.8374\n[1,2]&lt;stdout&gt;: - 0s - loss: 0.8910\n[1,2]&lt;stdout&gt;:Epoch 186/200\n[1,5]&lt;stdout&gt;: - 0s - loss: 0.9418\n[1,5]&lt;stdout&gt;:Epoch 180/200\n[1,0]&lt;stdout&gt;: - 0s - loss: 0.9419\n[1,0]&lt;stdout&gt;:Epoch 186/200\n[1,1]&lt;stdout&gt;: - 0s - loss: 0.8268\n[1,1]&lt;stdout&gt;:Epoch 200/200\n[1,2]&lt;stdout&gt;: - 0s - loss: 0.9023\n[1,2]&lt;stdout&gt;:Epoch 187/200\n[1,6]&lt;stdout&gt;: - 0s - loss: 0.9052\n[1,6]&lt;stdout&gt;:Epoch 172/200\n[1,5]&lt;stdout&gt;: - 0s - loss: 0.9610\n[1,5]&lt;stdout&gt;:Epoch 181/200\n[1,1]&lt;stdout&gt;: - 0s - loss: 0.9098\n[1,0]&lt;stdout&gt;: - 0s - loss: 0.9702\n[1,0]&lt;stdout&gt;:Epoch 187/200\n[1,2]&lt;stdout&gt;: - 0s - loss: 0.8819\n[1,2]&lt;stdout&gt;:Epoch 188/200\n[1,6]&lt;stdout&gt;: - 0s - loss: 0.8925\n[1,6]&lt;stdout&gt;:Epoch 173/200\n[1,5]&lt;stdout&gt;: - 0s - loss: 0.9484\n[1,5]&lt;stdout&gt;:Epoch 182/200\n[1,0]&lt;stdout&gt;: - 0s - loss: 0.9693\n[1,0]&lt;stdout&gt;:Epoch 188/200\n[1,2]&lt;stdout&gt;: - 0s - loss: 0.9040\n[1,2]&lt;stdout&gt;:Epoch 189/200\n[1,6]&lt;stdout&gt;: - 0s - loss: 0.8865\n[1,6]&lt;stdout&gt;:Epoch 174/200\n[1,5]&lt;stdout&gt;: - 0s - loss: 0.9283\n[1,5]&lt;stdout&gt;:Epoch 183/200\n[1,0]&lt;stdout&gt;: - 0s - loss: 0.9268\n[1,0]&lt;stdout&gt;:Epoch 189/200\n[1,2]&lt;stdout&gt;: - 0s - loss: 0.8689\n[1,2]&lt;stdout&gt;:Epoch 190/200\n[1,6]&lt;stdout&gt;: - 0s - loss: 0.8910\n[1,6]&lt;stdout&gt;:Epoch 175/200\n[1,5]&lt;stdout&gt;: - 0s - loss: 0.9023\n[1,5]&lt;stdout&gt;:Epoch 184/200\n[1,0]&lt;stdout&gt;: - 0s - loss: 0.9750\n[1,0]&lt;stdout&gt;:Epoch 190/200\n[1,2]&lt;stdout&gt;: - 0s - loss: 0.8507\n[1,2]&lt;stdout&gt;:Epoch 191/200\n[1,5]&lt;stdout&gt;: - 0s - loss: 0.9088\n[1,5]&lt;stdout&gt;:Epoch 185/200\n[1,6]&lt;stdout&gt;: - 0s - loss: 0.9263\n[1,6]&lt;stdout&gt;:Epoch 176/200\n[1,0]&lt;stdout&gt;: - 0s - loss: 0.9421\n[1,0]&lt;stdout&gt;:Epoch 191/200\n[1,2]&lt;stdout&gt;: - 0s - loss: 0.8161\n[1,2]&lt;stdout&gt;:Epoch 192/200\n[1,5]&lt;stdout&gt;: - 0s - loss: 0.9618\n[1,5]&lt;stdout&gt;:Epoch 186/200\n[1,0]&lt;stdout&gt;: - 0s - loss: 0.9516\n[1,0]&lt;stdout&gt;:Epoch 192/200\n[1,6]&lt;stdout&gt;: - 0s - loss: 0.8759\n[1,6]&lt;stdout&gt;:Epoch 177/200\n[1,2]&lt;stdout&gt;: - 0s - loss: 0.8773\n[1,2]&lt;stdout&gt;:Epoch 193/200\n[1,5]&lt;stdout&gt;: - 0s - loss: 0.9081\n[1,5]&lt;stdout&gt;:Epoch 187/200\n[1,0]&lt;stdout&gt;: - 0s - loss: 0.8991\n[1,0]&lt;stdout&gt;:Epoch 193/200\n[1,6]&lt;stdout&gt;: - 0s - loss: 0.8943\n[1,6]&lt;stdout&gt;:Epoch 178/200\n[1,2]&lt;stdout&gt;: - 0s - loss: 0.8256\n[1,2]&lt;stdout&gt;:Epoch 194/200\n[1,5]&lt;stdout&gt;: - 0s - loss: 0.9507\n[1,5]&lt;stdout&gt;:Epoch 188/200\n[1,0]&lt;stdout&gt;: - 0s - loss: 0.8618\n[1,0]&lt;stdout&gt;:Epoch 194/200\n[1,2]&lt;stdout&gt;: - 0s - loss: 0.8985\n[1,2]&lt;stdout&gt;:Epoch 195/200\n[1,6]&lt;stdout&gt;: - 0s - loss: 0.9400\n[1,6]&lt;stdout&gt;:Epoch 179/200\n[1,5]&lt;stdout&gt;: - 0s - loss: 0.8867\n[1,5]&lt;stdout&gt;:Epoch 189/200\n[1,0]&lt;stdout&gt;: - 0s - loss: 0.9423\n[1,0]&lt;stdout&gt;:Epoch 195/200\n[1,2]&lt;stdout&gt;: - 0s - loss: 0.8769\n[1,2]&lt;stdout&gt;:Epoch 196/200\n[1,6]&lt;stdout&gt;: - 0s - loss: 0.9254\n[1,6]&lt;stdout&gt;:Epoch 180/200\n[1,0]&lt;stdout&gt;: - 0s - loss: 0.9027\n[1,0]&lt;stdout&gt;:Epoch 196/200\n[1,5]&lt;stdout&gt;: - 0s - loss: 0.9495\n[1,5]&lt;stdout&gt;:Epoch 190/200\n[1,2]&lt;stdout&gt;: - 0s - loss: 0.8891\n[1,2]&lt;stdout&gt;:Epoch 197/200\n[1,6]&lt;stdout&gt;: - 0s - loss: 0.9056\n[1,6]&lt;stdout&gt;:Epoch 181/200\n[1,0]&lt;stdout&gt;: - 0s - loss: 0.8948\n[1,0]&lt;stdout&gt;:Epoch 197/200\n[1,2]&lt;stdout&gt;: - 0s - loss: 0.8374\n[1,2]&lt;stdout&gt;:Epoch 198/200\n[1,6]&lt;stdout&gt;: - 0s - loss: 0.8599\n[1,6]&lt;stdout&gt;:Epoch 182/200\n[1,5]&lt;stdout&gt;: - 0s - loss: 0.9359\n[1,5]&lt;stdout&gt;:Epoch 191/200\n[1,0]&lt;stdout&gt;: - 0s - loss: 0.9790\n[1,0]&lt;stdout&gt;:Epoch 198/200\n[1,2]&lt;stdout&gt;: - 0s - loss: 0.8420\n[1,2]&lt;stdout&gt;:Epoch 199/200\n[1,5]&lt;stdout&gt;: - 0s - loss: 0.9142\n[1,5]&lt;stdout&gt;:Epoch 192/200\n[1,6]&lt;stdout&gt;: - 0s - loss: 0.8753\n[1,6]&lt;stdout&gt;:Epoch 183/200\n[1,0]&lt;stdout&gt;: - 0s - loss: 0.9352\n[1,0]&lt;stdout&gt;:Epoch 199/200\n[1,2]&lt;stdout&gt;: - 0s - loss: 0.8750\n[1,2]&lt;stdout&gt;:Epoch 200/200\n[1,5]&lt;stdout&gt;: - 0s - loss: 0.9328\n[1,5]&lt;stdout&gt;:Epoch 193/200\n[1,6]&lt;stdout&gt;: - 0s - loss: 0.8418\n[1,6]&lt;stdout&gt;:Epoch 184/200\n[1,2]&lt;stdout&gt;: - 0s - loss: 0.8009\n[1,0]&lt;stdout&gt;: - 0s - loss: 0.8723\n[1,0]&lt;stdout&gt;:Epoch 200/200\n[1,5]&lt;stdout&gt;: - 0s - loss: 0.8952\n[1,5]&lt;stdout&gt;:Epoch 194/200\n[1,6]&lt;stdout&gt;: - 0s - loss: 0.8584\n[1,6]&lt;stdout&gt;:Epoch 185/200\n[1,0]&lt;stdout&gt;: - 0s - loss: 0.8806\n[1,5]&lt;stdout&gt;: - 0s - loss: 0.9576\n[1,5]&lt;stdout&gt;:Epoch 195/200\n[1,6]&lt;stdout&gt;: - 0s - loss: 0.8335\n[1,6]&lt;stdout&gt;:Epoch 186/200\n[1,5]&lt;stdout&gt;: - 0s - loss: 0.8898\n[1,5]&lt;stdout&gt;:Epoch 196/200\n[1,6]&lt;stdout&gt;: - 0s - loss: 0.8634\n[1,6]&lt;stdout&gt;:Epoch 187/200\n[1,5]&lt;stdout&gt;: - 0s - loss: 0.9527\n[1,5]&lt;stdout&gt;:Epoch 197/200\n[1,6]&lt;stdout&gt;: - 0s - loss: 0.8279\n[1,6]&lt;stdout&gt;:Epoch 188/200\n[1,5]&lt;stdout&gt;: - 0s - loss: 0.9155\n[1,5]&lt;stdout&gt;:Epoch 198/200\n[1,6]&lt;stdout&gt;: - 0s - loss: 0.8417\n[1,6]&lt;stdout&gt;:Epoch 189/200\n[1,5]&lt;stdout&gt;: - 0s - loss: 0.9794\n[1,5]&lt;stdout&gt;:Epoch 199/200\n[1,6]&lt;stdout&gt;: - 0s - loss: 0.8649\n[1,6]&lt;stdout&gt;:Epoch 190/200\n[1,5]&lt;stdout&gt;: - 0s - loss: 0.9198\n[1,5]&lt;stdout&gt;:Epoch 200/200\n[1,6]&lt;stdout&gt;: - 0s - loss: 0.9186\n[1,6]&lt;stdout&gt;:Epoch 191/200\n[1,5]&lt;stdout&gt;: - 0s - loss: 0.9099\n[1,6]&lt;stdout&gt;: - 0s - loss: 0.8185\n[1,6]&lt;stdout&gt;:Epoch 192/200\n[1,6]&lt;stdout&gt;: - 0s - loss: 0.8231\n[1,6]&lt;stdout&gt;:Epoch 193/200\n[1,6]&lt;stdout&gt;: - 0s - loss: 0.8098\n[1,6]&lt;stdout&gt;:Epoch 194/200\n[1,6]&lt;stdout&gt;: - 0s - loss: 0.8076\n[1,6]&lt;stdout&gt;:Epoch 195/200\n[1,6]&lt;stdout&gt;: - 0s - loss: 0.8489\n[1,6]&lt;stdout&gt;:Epoch 196/200\n[1,6]&lt;stdout&gt;: - 0s - loss: 0.7928\n[1,6]&lt;stdout&gt;:Epoch 197/200\n[1,6]&lt;stdout&gt;: - 0s - loss: 0.8485\n[1,6]&lt;stdout&gt;:Epoch 198/200\n[1,6]&lt;stdout&gt;: - 0s - loss: 0.7960\n[1,6]&lt;stdout&gt;:Epoch 199/200\n[1,6]&lt;stdout&gt;: - 0s - loss: 0.8811\n[1,6]&lt;stdout&gt;:Epoch 200/200\n[1,6]&lt;stdout&gt;: - 0s - loss: 0.8341\ntime cost is 29.3283953666687\n</div>"]}}],"execution_count":10},{"cell_type":"code","source":["X, A, y = load_data(path=PATH,dataset=DATASET)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Loading cora dataset...\nDataset has 2708 nodes, 5429 edges, 1433 features.\n</div>"]}}],"execution_count":11},{"cell_type":"code","source":["#more about pickle\n#https://stackoverflow.com/questions/44144584/typeerror-cant-pickle-thread-lock-objects\n#https://github.com/keras-team/keras/issues/8343\n#https://github.com/keras-team/keras/issues/10528\n#https://bugs.python.org/issue29168\n#https://www.reddit.com/r/learnpython/comments/bl2vze/what_does_the_error_message_typeerror_cant_pickle/\n#https://www.reddit.com/r/learnpython/comments/brbplc/cant_pickle_threadrlock_objects/\n#https://www.bountysource.com/issues/66242430-typeerror-can-t-pickle-_thread-rlock-objects-while-saving-the-keras-model-using-model-save\n#https://devrant.com/rants/1279739/typeerror-cant-pickle-thread-lock-objects-someone-please-guide-me-pick-dump-mode\n#https://bugzilla.redhat.com/show_bug.cgi?id=1444983"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":12},{"cell_type":"code","source":["# Testing\ntest_loss, test_acc = evaluate_preds(preds, [y_test], [idx_test])\nprint(\"Test set results:\",\n      \"loss= {:.4f}\".format(test_loss[0]),\n      \"accuracy= {:.4f}\".format(test_acc[0]))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":13}],"metadata":{"name":"train_horovod","notebookId":18092},"nbformat":4,"nbformat_minor":0}
