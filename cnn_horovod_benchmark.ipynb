{"cells":[{"cell_type":"code","source":["### get current timestamp\nimport datetime\nfrom pytz import timezone\n\ncurent_timestamp = datetime.datetime.now().astimezone(timezone('US/Pacific')).strftime(\"%Y_%m_%d_%H_%M_%S\")\n\n### Create horovod_benchmark folder under mounted s3 bucket\nimport os\nfinal_output_dir = \"/dbfs/mnt/wendao_test2/horovod_benchmark/logs/\"\noutput_dir = 'logs/logs/horovod_logs/'\n\noutput_dir = output_dir + f\"{curent_timestamp}/\"\nmodel_output_dir = output_dir + \"model/\"\n\nfor _path in [output_dir, final_output_dir, model_output_dir]:\n  try:\n    os.makedirs(_path)\n    print(f\"{_path} - file created successfully\")\n  except Exception as ex:\n    if FileExistsError:\n      print(f\"{_path} - file already exist\")\n    else:\n      raise ex"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">logs/logs/horovod_logs/2019_11_13_23_12_23/ - file created successfully\n/dbfs/mnt/wendao_test2/horovod_benchmark/logs/ - file already exist\nlogs/logs/horovod_logs/2019_11_13_23_12_23/model/ - file created successfully\n</div>"]}}],"execution_count":1},{"cell_type":"code","source":["import logging\nimport sys\nimport datetime\nimport glob\n\n\n### TO DO \n### ADDING SINGLE_INSTANCE, REPEAT INTO LOGGER STRING FORMAT\noriginal_stdout = sys.stdout\noriginal_stderr = sys.stderr\n\ndef redirect_stdout(log_filename):\n  ### Databricks has weird behavior that need to define redirect stdout in every cell. -- Need follow up with databricks\n  ### https://stackoverflow.com/questions/34248908/how-to-prevent-logging-of-pyspark-answer-received-and-command-to-send-messag\n  ### logging flushed with py4j error messages so to set logging level to ERROR to silence error\n  ### Also leave pyspark/matplotlib silence code just in case\n  logging.getLogger(\"py4j\").setLevel(logging.ERROR)\n  ##logging.getLogger('pyspark').setLevel(logging.ERROR)\n  ##matplotlib_logger = logging.getLogger(\"matplotlib\").setLevel(logging.ERROR)\n  \n  class StreamToLogger:\n    def __init__(self, logger, level):\n        # self.level is really like using log.debug(message)\n        # at least in my case\n        self.level = level\n        self.logger = logger\n\n    def write(self, message):\n        # if statement reduces the amount of newlines that are\n        # printed to the logger\n        if (message != '\\n') & (message != ''):\n          for mess in message.splitlines():\n            self.logger.log(self.level, mess.rstrip())\n            \n    def flush(self):\n      pass\n#     def flush(self):\n#         # create a flush method so things can be flushed when\n#         # the system wants to. Not sure if simply 'printing'\n#         # sys.stderr is the correct way to do it, but it seemed\n#         # to work properly for me.\n#         self.level(sys.stderr)\n        \n  ## Change to pdt timezone (Simply implementation won't consider daylight savings)\n  def pdt_timezone(sec, what):\n    pdt_time = datetime.datetime.now() - datetime.timedelta(hours=8)\n    return pdt_time.timetuple()\n  logging.Formatter.converter = pdt_timezone\n  \n    \n  logging.basicConfig(\n     level=logging.DEBUG,\n     ## format='%(asctime)-%(levelname)s-%(name)s-%(message)s',\n     format=('%(asctime)s - %(name)s - %(levelname)s - %(message)s'),\n     datefmt=\"%Y-%m-%d %H:%M:%S\",\n     filename = log_filename,\n     filemode = 'w'\n  )\n         \n  stdout_logger = logging.getLogger('STDOUT')\n  sl = StreamToLogger(stdout_logger, logging.INFO)\n  sys.stdout = sl\n  \n  stderr_logger = logging.getLogger('STDERR')\n  sle = StreamToLogger(stderr_logger, logging.ERROR)\n  sys.stderr = sle\n  \ndef reset_stdout():\n  sys.stdout = original_stdout\n  sys.stderr = original_stderr\n  temp_logger = logging.getLogger()\n  temp_logger.handlers = []\n  \n  \ndef move_log_to_s3():\n  ### Moving all the driver logs to mounted s3 bucket. \n  import glob\n  from distutils import dir_util \n  ### Shutile will not overwrit the file so use distutils instead\n\n  list_dir = glob.glob(output_dir)\n\n  for path in list_dir:\n    dir_name = os.path.basename(os.path.dirname(path))\n    dest = final_output_dir + dir_name\n    destination = dir_util.copy_tree(path, dest) \n\ndef save_model_single(filename):\n  import shutil\n  dest_dir = output_dir + str(np_setup) + \"/\" \n  if not os.path.exists(dest_dir):\n    os.makedirs(dest_dir)\n  shutil.copy(filename, dest_dir + filename)\n  \ndef save_horovod_model():\n  import shutil\n  for path in glob.glob(checkpoint_dir+\"/*\"):\n    desc_dir = model_output_dir + \"NP\" +str(np_setup) + \"/\"\n    if not os.path.exists(desc_dir):\n      os.makedirs(desc_dir)\n    model_filename = os.path.basename(path)\n    shutil.copy(path, desc_dir + model_filename)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":2},{"cell_type":"code","source":["dbutils.widgets.removeAll()\ndbutils.widgets.text(name = \"batch_size\", defaultValue = \"128\")\ndbutils.widgets.text(name = \"learning_rate\", defaultValue = \"0.1\")\ndbutils.widgets.text(name = \"epochs\", defaultValue = \"5\")\ndbutils.widgets.text(name = \"repeat\", defaultValue = \"3\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":3},{"cell_type":"code","source":["batch_size = int(dbutils.widgets.get(\"batch_size\"))\nepochs = int(dbutils.widgets.get(\"epochs\"))\nrepeat = int(dbutils.widgets.get(\"repeat\"))\nlearning_rate = float(dbutils.widgets.get(\"learning_rate\"))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":4},{"cell_type":"code","source":["import time\nimport os\nimport datetime\nimport time\nimport pandas as pd \nimport numpy as np\n\ncheckpoint_dir = '/dbfs/ml/MNISTDemo/train/{}/'.format(time.time())\n\nos.makedirs(checkpoint_dir)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":5},{"cell_type":"code","source":["# batch_size = 128\n# epochs = 5\nnum_classes = 10"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":6},{"cell_type":"code","source":["#tf.keras\nfrom tensorflow import keras\nfrom tensorflow.keras import models\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Dense,Dropout,GlobalAveragePooling2D\n\nimport horovod.tensorflow.keras as hvd\nfrom tensorflow.keras import backend as K\nimport tensorflow as tf\n#np.set_printoptions(threshold=np.inf)\n\n#https://pypi.org/project/keras-rectified-adam/\n# from keras_radam import RAdam"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">/databricks/python/lib/python3.7/site-packages/tensorflow/python/util/nest.py:1286: DeprecationWarning: Using or importing the ABCs from &#39;collections&#39; instead of from &#39;collections.abc&#39; is deprecated, and in 3.8 it will stop working\n  _pywrap_tensorflow.RegisterType(&#34;Mapping&#34;, _collections.Mapping)\nWARNING:tensorflow:From /databricks/python/lib/python3.7/site-packages/horovod/tensorflow/__init__.py:117: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n\nWARNING:tensorflow:From /databricks/python/lib/python3.7/site-packages/horovod/tensorflow/__init__.py:143: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n\n</div>"]}}],"execution_count":7},{"cell_type":"code","source":["\n\ndef get_dataset(num_classes, rank=0, size=1):\n  (x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data('MNIST-data-%d' % rank)\n  x_train = x_train[rank::size]\n  y_train = y_train[rank::size]\n  x_test = x_test[rank::size]\n  y_test = y_test[rank::size]\n  x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n  x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n  x_train = x_train.astype('float32')\n  x_test = x_test.astype('float32')\n  x_train /= 255\n  x_test /= 255\n  y_train = keras.utils.to_categorical(y_train, num_classes)\n  y_test = keras.utils.to_categorical(y_test, num_classes)\n  return (x_train, y_train), (x_test, y_test)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":8},{"cell_type":"code","source":["from tensorflow.keras import models\nfrom tensorflow.keras import layers\n\ndef get_model(num_classes):\n  model = models.Sequential()\n  model.add(layers.Conv2D(32, kernel_size=(3, 3),\n                   activation='relu',\n                   input_shape=(28, 28, 1)))\n  model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n  model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n  model.add(layers.Dropout(0.25))\n  model.add(layers.Flatten())\n  model.add(layers.Dense(128, activation='relu'))\n  model.add(layers.Dropout(0.5))\n  model.add(layers.Dense(num_classes, activation='softmax'))\n  return model"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":9},{"cell_type":"code","source":["def train(learning_rate=1.0):\n  (x_train, y_train), (x_test, y_test) = get_dataset(num_classes)\n  model = get_model(num_classes)\n\n  optimizer = keras.optimizers.Adadelta(lr=learning_rate)\n\n  model.compile(optimizer=optimizer,\n                loss='categorical_crossentropy',\n                metrics=['accuracy'])\n\n  model.fit(x_train, y_train,\n            batch_size=batch_size,\n            epochs=epochs,\n            verbose=2,\n            validation_data=(x_test, y_test))\n  \n  return model\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":10},{"cell_type":"code","source":["### Define output log file name \nenv = \"PROD\"\ninstance_type = 'SINGLE_INSTANCE'\ndataset = 'MNIST'\nmodel_name = 'CNN'\n\n(x_train, y_train), (x_test, y_test) = get_dataset(num_classes)\n\ntrain_shape = x_train.shape[0]\nvalidation_shape = x_test.shape[0]\n\ndef get_cluster_info(hvd_run=True):\n  driver_type = sc.getConf().get(\"spark.databricks.driverNodeTypeId\")\n  worker_type = \"None\"\n  num_workers = \"None\"\n  if hvd_run == True:\n    worker_type = sc.getConf().get(\"spark.databricks.workerNodeTypeId\")\n    num_workers = sc.getConf().get(\"spark.databricks.clusterUsageTags.clusterWorkers\")\n  return [driver_type, worker_type, num_workers]\n\ndriver_type, worker_type, num_workers = get_cluster_info()\nnp_setup = \"NA\""],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n\r    8192/11490434 [..............................] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r  983040/11490434 [=&gt;............................] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 8822784/11490434 [======================&gt;.......] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r11493376/11490434 [==============================] - 0s 0us/step\n</div>"]}}],"execution_count":11},{"cell_type":"code","source":["filename = f\"benchmark|{env}|{dataset}|{model_name}|{train_shape}|{validation_shape}|{instance_type}|{driver_type}|{worker_type}|{num_workers}|{np_setup}|{repeat}|{epochs}|{learning_rate}|{batch_size}.log\""],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":12},{"cell_type":"code","source":["reset_stdout()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":13},{"cell_type":"code","source":["print(output_dir+filename)\n\nredirect_stdout(output_dir+filename)\nfor i in range(repeat):\n  print(f\"REPEAT {i+1}\")\n  model = train()\n  model_file_name = filename.split(\".log\")[0] + \".h5\"\n  model.save(model_file_name)\nsave_model_single(model_file_name)\nmove_log_to_s3()\nreset_stdout()\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">logs/logs/horovod_logs/2019_11_13_11_09_19/benchmark|PROD|MNIST|CNN|60000|10000|SINGLE_INSTANCE|c4.2xlarge|c4.2xlarge|8|NA|3|50|0.1|128.log\nWARNING:tensorflow:From /databricks/python/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\nInstructions for updating:\nCall initializer instance with the dtype argument instead of passing it to the constructor\n</div>"]}}],"execution_count":14},{"cell_type":"code","source":["save_model_single(model_file_name)\nmove_log_to_s3()\nreset_stdout()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":15},{"cell_type":"code","source":["%sh ls /dbfs/mnt/wendao_test2/horovod_benchmark/logs/2019_11_12_14_56_52"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">NA\nbenchmark|PROD|MNIST|CNN|60000|10000|SINGLE_INSTANCE|c4.2xlarge|c4.2xlarge|8|NA|1|50|0.1|128.log\n</div>"]}}],"execution_count":16},{"cell_type":"code","source":["#fit_generator version of hvd\n#https://github.com/horovod/horovod/blob/master/examples/keras_mnist_advanced.py\n#btw, the advanced example used fancy adadelta optimizer, that's why there are so many call backs.\n#https://docs.databricks.com/applications/deep-learning/distributed-training/mnist-tensorflow-keras.html\n#my understanding is that the entire script of horovod exampe needs to be put in train_hvd fundtion\n\ndef train_hvd(learning_rate = learning_rate):\n  # Horovod: initialize Horovod.\n  hvd.init()\n\n  # Horovod: pin GPU to be used to process local rank (one GPU per process)\n#   config = tf.ConfigProto()\n#   config.gpu_options.allow_growth = True\n#   config.gpu_options.visible_device_list = str(hvd.local_rank())\n#   K.set_session(tf.Session(config=config))\n\n  \n  # Collect get data time\n  start_time = time.time()\n  \n  (x_train, y_train), (x_test, y_test) = get_dataset(num_classes, hvd.rank(), hvd.size())\n  \n  end_time = round((time.time() - start_time),3)\n  print(f\"step - get_data - {end_time}\")  \n\n  \n  # Model Compiles\n  start_time = time.time()\n\n#   model=get_model()\n  model = get_model(num_classes)\n  \n  # Horovod: adjust learning rate based on number of GPUs.\n  optimizer = keras.optimizers.Adadelta(lr=learning_rate * hvd.size())\n#   optimizer = keras.optimizers.Adadelta(lr=learning_rate * hvd.size())\n  \n#   optimizer =RAdam(total_steps=5000, warmup_proportion=0.1, learning_rate=learning_rate*hvd.size(), min_lr=1e-5)\n\n  # Horovod: add Horovod Distributed Optimizer.\n  optimizer = hvd.DistributedOptimizer(optimizer)\n\n  model.compile(optimizer=optimizer,\n                loss='categorical_crossentropy',\n                metrics=['accuracy'])\n\n  callbacks = [\n      # Horovod: broadcast initial variable states from rank 0 to all other processes.\n      # This is necessary to ensure consistent initialization of all workers when\n      # training is started with random weights or restored from a checkpoint.\n      hvd.callbacks.BroadcastGlobalVariablesCallback(0),\n#       hvd.callbacks.MetricAverageCallback(),\n#       hvd.callbacks.LearningRateWarmupCallback(warmup_epochs=5, verbose=1),\n#       keras.callbacks.ReduceLROnPlateau(patience=10, verbose=1)\n\n  ]\n\n  # Horovod: save checkpoints only on worker 0 to prevent other workers from corrupting them.\n  if hvd.rank() == 0:\n      callbacks.append(keras.callbacks.ModelCheckpoint(checkpoint_dir + 'checkpoint-{epoch}.ckpt', save_weights_only = True))\n\n  end_time = round((time.time() - start_time),3)\n  print(f\"step - prep_model - {end_time}\") \n  \n  # Model Train\n  start_time = time.time()\n  \n  model.fit(x_train, y_train,\n              batch_size=batch_size,\n              callbacks=callbacks,\n              epochs=epochs,\n              verbose=2,\n              validation_data=(x_test, y_test)\n         )\n   \n  \n  end_time = round((time.time() - start_time),3)\n  print(f\"step - train_model - {end_time}\")   \n    "],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":17},{"cell_type":"code","source":["from sparkdl import HorovodRunner\ninstance_type = \"HOROVOD_CLUSTER\""],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Using TensorFlow backend.\n</div>"]}}],"execution_count":18},{"cell_type":"code","source":["np_list = [-8, 1, 2, 4, 8, 16, 32]"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":19},{"cell_type":"code","source":["np_list = [8, 16, 32]"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":20},{"cell_type":"code","source":["for np_setup in np_list:\n  \n  start_time = time.time()\n  reset_stdout()\n  \n  checkpoint_dir = '/dbfs/ml/MNISTDemo/train/{}/{}/'.format(np_setup, time.time())\n  os.makedirs(checkpoint_dir)\n  \n  filename = f\"benchmark|{env}|{dataset}|{model_name}|{train_shape}|{validation_shape}|{instance_type}|{driver_type}|{worker_type}|{num_workers}|{np_setup}|{repeat}|{epochs}|{learning_rate}|{batch_size}.log\"\n  print(output_dir+filename)\n  redirect_stdout(output_dir+filename)\n\n  for i in range(repeat):\n    print(f\"{instance_type} - REPEAT {i+1}\")\n    hr = HorovodRunner(np = np_setup)\n    hr.run(train_hvd,  learning_rate=learning_rate)\n\n  reset_stdout()\n  end_time = round((time.time() - start_time),3)\n  print(f\"np{np_setup} - finshed in {end_time}\") \n  \n  print(\"Saving to s3....\")\n  save_horovod_model()\n  move_log_to_s3()\n  print(\"Saving to s3 finshed!\")\n  \n  reset_stdout()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">logs/logs/horovod_logs/2019_11_13_23_12_23/benchmark|PROD|MNIST|CNN|60000|10000|HOROVOD_CLUSTER|c4.2xlarge|c4.2xlarge|8|8|3|50|0.1|128.log\nnp8 - finshed in 1433.82\nSaving to s3....\nSaving to s3 finshed!\nlogs/logs/horovod_logs/2019_11_13_23_12_23/benchmark|PROD|MNIST|CNN|60000|10000|HOROVOD_CLUSTER|c4.2xlarge|c4.2xlarge|8|16|3|50|0.1|128.log\nnp16 - finshed in 1201.547\nSaving to s3....\nSaving to s3 finshed!\nlogs/logs/horovod_logs/2019_11_13_23_12_23/benchmark|PROD|MNIST|CNN|60000|10000|HOROVOD_CLUSTER|c4.2xlarge|c4.2xlarge|8|32|3|50|0.1|128.log\nnp32 - finshed in 1171.9\nSaving to s3....\nSaving to s3 finshed!\n</div>"]}}],"execution_count":21}],"metadata":{"name":"cnn_horovod_benchmark","notebookId":18057},"nbformat":4,"nbformat_minor":0}
